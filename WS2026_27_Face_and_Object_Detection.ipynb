{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IPHOS-RWU/3DMV/blob/main/WS2026_27_Face_and_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1O_MD9kFcZ5"
      },
      "source": [
        "Vorbereitung: Bilder von Github laden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPqmhUePFNcl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/IPHOS-RWU/3DMV/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ko3wxrFmqx"
      },
      "source": [
        "Vorbereitung: Libraries importieren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pO9s9MNFoaM"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "print(cv2.__version__)\n",
        "%cd 3DMV/images\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-KzZfVk-MAl"
      },
      "outputs": [],
      "source": [
        "nadia = cv2.imread('Nadia_Murad.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "denis = cv2.imread('Denis_Mukwege.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "solvay = cv2.imread('solvay_conference.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "print(nadia.shape)\n",
        "print(denis.shape)\n",
        "print(solvay.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbDH5x5q-fOQ"
      },
      "outputs": [],
      "source": [
        "plt.imshow(nadia,cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srI4bBXb-t3w"
      },
      "source": [
        "**Cascade Files in OpenCV**\n",
        "\n",
        "Cascade Files sind fertig trainierte Modelle in OpenCV, die haeufig zur Gesichtserkennung eingesetzt werden.\n",
        "Sie liegen als XML-Dateien vor und koennen direkt verwendet werden.\n",
        "\n",
        "Typische Cascade Files erkennen:\n",
        "- Gesichter\n",
        "- Augen\n",
        "- Laecheln\n",
        "\n",
        "**Wie arbeitet ein Cascade Classifier?**\n",
        "Das Bild wird an vielen Stellen und in unterschiedlichen Groessen untersucht.\n",
        "Dabei sucht das Modell nach einfachen Mustern wie Kanten und Kontrasten, die typisch fuer Gesichter sind.\n",
        "\n",
        "Bildbereiche, die offensichtlich kein Gesicht enthalten, werden frueh verworfen.\n",
        "So bleibt die Berechnung schnell.\n",
        "\n",
        "**Vorteile**\n",
        "- Sehr schnell\n",
        "- Einfach zu verwenden\n",
        "- Gut geeignet fuer Echtzeit-Anwendungen\n",
        "\n",
        "**Einschraenkungen**\n",
        "- Empfindlich gegen Lichtveraenderungen\n",
        "- Weniger robust als moderne Deep-Learning-Modelle\n",
        "\n",
        "**Einordnung**\n",
        "Heute werden oft neuronale Netze eingesetzt. Cascade Files sind trotzdem gut geeignet, um die Grundidee der Gesichtserkennung zu verstehen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI2jJfOa-g8J"
      },
      "outputs": [],
      "source": [
        "# laedt das vortrainierte Haar-Cascade-Modell fuer die Gesichtserkennung\n",
        "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgxCZHE1-8Uc"
      },
      "outputs": [],
      "source": [
        "# erkennt Gesichter und zeichnet Rechtecke um sie\n",
        "face_img = denis.copy()\n",
        "\n",
        "face_rects = face_cascade.detectMultiScale(face_img)\n",
        "\n",
        "for (x,y,w,h) in face_rects:\n",
        "  cv2.rectangle(face_img, (x,y), (x+w,y+h), 255, 10)\n",
        "\n",
        "plt.imshow(face_img,cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWn_-2yNACJY"
      },
      "outputs": [],
      "source": [
        "# erkennt Gesicht Nadia und markiert es mit Rechteck\n",
        "face_img = nadia.copy()\n",
        "\n",
        "face_rects = face_cascade.detectMultiScale(face_img)\n",
        "\n",
        "for (x,y,w,h) in face_rects:\n",
        "  cv2.rectangle(face_img, (x,y), (x+w,y+h), 255, 10)\n",
        "\n",
        "plt.imshow(face_img,cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6lD5ukc_Ron"
      },
      "outputs": [],
      "source": [
        "# jetzt mit vielen Gesichtern\n",
        "face_img = solvay.copy()\n",
        "\n",
        "face_rects = face_cascade.detectMultiScale(face_img)\n",
        "\n",
        "for (x,y,w,h) in face_rects:\n",
        "  cv2.rectangle(face_img, (x,y), (x+w,y+h), 255, 10)\n",
        "\n",
        "plt.imshow(face_img,cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV6ciW5d_x4Q"
      },
      "source": [
        "**Anpassung der Parameter bei Haar-Cascade-Erkennung**\n",
        "\n",
        "Die Gesichtserkennung kann ueber einige Parameter angepasst werden.\n",
        "\n",
        "**scaleFactor** bestimmt, wie stark das Bild bei jedem Schritt verkleinert wird.\n",
        "Kleinere Werte sind genauer, aber langsamer.\n",
        "\n",
        "**minNeighbors** legt fest, wie sicher eine Erkennung sein muss.\n",
        "Hoehere Werte reduzieren Fehlalarme.\n",
        "\n",
        "**minSize** setzt eine Mindestgroesse fuer erkannte Gesichter.\n",
        "Sehr kleine Objekte werden ignoriert.\n",
        "\n",
        "**maxSize** ist optional und begrenzt die maximale Groesse eines Gesichts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOxq45uc_mmw"
      },
      "outputs": [],
      "source": [
        "# scaleFactor=1.15, minNeighbors=4\n",
        "\n",
        "face_img = solvay.copy()\n",
        "\n",
        "face_rects = face_cascade.detectMultiScale(face_img, scaleFactor=1.15, minNeighbors=4)\n",
        "\n",
        "for (x,y,w,h) in face_rects:\n",
        "  cv2.rectangle(face_img, (x,y), (x+w,y+h), 255, 10)\n",
        "\n",
        "plt.imshow(face_img,cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNkCO4QAAnCf"
      },
      "source": [
        "**Smile Detektor**\n",
        "Der Smile-Detektor in OpenCV basiert auf der Haar-Cascade-Datei haarcascade_smile.xml. Er erkennt Lächeln, indem er charakteristische Muster im Bereich des Mundes analysiert.\n",
        "Der Smile-Detektor ist weniger robust als die Gesichtserkennung und anfällig für Variationen wie Licht, Winkel oder verdeckte Gesichter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJG4IFVNAlhQ"
      },
      "outputs": [],
      "source": [
        "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
        "eye_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_eye.xml')\n",
        "smile_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_smile.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DClFb6gtA4B_"
      },
      "outputs": [],
      "source": [
        "nadia = cv2.imread('Nadia_Murad.jpg')\n",
        "img = cv2.cvtColor(nadia, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gvirc1F9BQou"
      },
      "outputs": [],
      "source": [
        "img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "faces = face_cascade.detectMultiScale(img_gray)\n",
        "for (x, y, w, h) in faces:\n",
        "  # Ausschnitte kopieren\n",
        "  roi_gray = img_gray[y:y+h, x:x+w]\n",
        "  roi_color = img[y:y+h, x:x+h]\n",
        "  # Gesicht markieren\n",
        "  cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "  # Nach Augen suchen\n",
        "  eyes = eye_cascade.detectMultiScale(roi_gray, 1.1, 22)\n",
        "  for (ex, ey, ew, eh) in eyes:\n",
        "    cv2.rectangle(roi_color, (ex, ey),(ex+ew, ey+eh), (0, 255, 0), 2)\n",
        "\n",
        "  # Nach Smile suchen\n",
        "  smiles = smile_cascade.detectMultiScale(roi_gray, 1.7, 22)\n",
        "  for (sx, sy, sw, sh) in smiles:\n",
        "    cv2.rectangle(roi_color, (sx, sy),(sx+sw, sy+sh), (0, 0, 255), 2)\n",
        "\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2nmpPyYB7JW"
      },
      "outputs": [],
      "source": [
        "therock = cv2.imread('therock.jpg')\n",
        "img = cv2.cvtColor(therock, cv2.COLOR_BGR2RGB)\n",
        "img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "faces = face_cascade.detectMultiScale(img_gray)\n",
        "for (x, y, w, h) in faces:\n",
        "  # Ausschnitte kopieren\n",
        "  roi_gray = img_gray[y:y+h, x:x+w]\n",
        "  roi_color = img[y:y+h, x:x+h]\n",
        "  # Gesicht markieren\n",
        "  cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "  # Nach Augen suchen\n",
        "  eyes = eye_cascade.detectMultiScale(roi_gray, 1.1, 22)\n",
        "  for (ex, ey, ew, eh) in eyes:\n",
        "    cv2.rectangle(roi_color, (ex, ey),(ex+ew, ey+eh), (0, 255, 0), 2)\n",
        "\n",
        "  # Nach Smile suchen\n",
        "  smiles = smile_cascade.detectMultiScale(roi_gray, 1.7, 22)\n",
        "  for (sx, sy, sw, sh) in smiles:\n",
        "    cv2.rectangle(roi_color, (sx, sy),(sx+sw, sy+sh), (0, 0, 255), 2)\n",
        "\n",
        "plt.imshow(img)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrqYjZtjFyks"
      },
      "source": [
        "**Facial Landmarks mit Dlib**\n",
        "\n",
        "Dlib bietet eine robuste Methode zur Erkennung von Gesichtslandmarks, also charakteristischen Punkten im Gesicht (z. B. Augen, Nase, Mund, Kinn). Das Modell identifiziert standardmäßig 68 Landmark-Punkte, die präzise die Gesichtskonturen markieren.\n",
        "\n",
        "Funktionsweise:\n",
        "- Nach der Gesichtserkennung lokalisiert Dlib mithilfe eines vortrainierten Modells (z. B. shape_predictor_68_face_landmarks.dat) die Landmark-Punkte.\n",
        "- Diese Punkte werden genutzt, um Gesichtszüge für Anwendungen wie Gesichtsausrichtung, Mimik-Analyse oder Augmented Reality zu verfolgen.\n",
        "\n",
        "Vorteile:\n",
        "- Präzise und robust bei unterschiedlichen Gesichtswinkeln.\n",
        "- Funktioniert gut unter moderaten Lichtbedingungen.\n",
        "\n",
        "Nachteile:\n",
        "- Höherer Rechenaufwand im Vergleich zu Haar-Cascades.\n",
        "- Kann bei extremen Gesichtspositionen oder verdeckten Bereichen an Genauigkeit verlieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1xoJR7gF0yO"
      },
      "outputs": [],
      "source": [
        "# Library dlib und imutils bereitsstellen\n",
        "import dlib\n",
        "import imutils\n",
        "from imutils import face_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLCHvWTvF6zP"
      },
      "outputs": [],
      "source": [
        "# Zunächst müssen wir die Modelldaten herunterladen\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbCLpkdsGGas"
      },
      "outputs": [],
      "source": [
        "# laedt das Gesichtsmodell und initialisiert die Gesichtserkennung\n",
        "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
        "detector = dlib.get_frontal_face_detector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4rL9I3jGKwL"
      },
      "outputs": [],
      "source": [
        "# laedt ein Bild und zeigt es an\n",
        "img = dlib.load_rgb_image('Nadia_Murad.jpg')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNoaVIFNGXii"
      },
      "outputs": [],
      "source": [
        "# erkennt Gesichter im Bild\n",
        "rects = detector(img, 1)\n",
        "print(\"Number of faces detected: {}\".format(len(rects)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0wgCI9sGb3-"
      },
      "outputs": [],
      "source": [
        "# berechnet Gesichtslandmarken und zeichnet Bounding Box und Punkte\n",
        "for i, rect in enumerate(rects):\n",
        "\n",
        "    # Gesichtslandmarken fuer dieses Gesicht berechnen\n",
        "    shape = predictor(img, rect)\n",
        "    shape = face_utils.shape_to_np(shape)\n",
        "\n",
        "    # Gesichtsbereich in eine Bounding Box umwandeln und zeichnen\n",
        "    (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
        "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    # Landmark-Punkte einzeichnen\n",
        "    for (x, y) in shape:\n",
        "        cv2.circle(img, (x, y), 5, (0, 0, 255), -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsK0bCH_GzRp"
      },
      "outputs": [],
      "source": [
        "# zeigt das Ergebnisbild an\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67PbPk1eHdOO"
      },
      "source": [
        "\n",
        "**Mediapipe Hands**\n",
        "\n",
        "updated: 2026-01-19\n",
        "\n",
        "MediaPipe Hands ist ein Framework zur Handerkennung und -verfolgung in Echtzeit, das die Position von 21 Landmarks pro Hand präzise erkennt. Es kombiniert eine effiziente Handsegmentierung mit Landmark-Vorhersagen, die für Anwendungen wie Gestenerkennung oder Interaktionen in AR/VR nützlich sind.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation Mediapipe\n",
        "\n",
        "!pip install -q mediapipe"
      ],
      "metadata": {
        "id": "IPLMb94KCFa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Daten aus dem Internet laden\n",
        "\n",
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
      ],
      "metadata": {
        "id": "gJVhh1K9CJh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "print(mp.__version__)"
      ],
      "metadata": {
        "id": "yyhU2f_jCTDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bild laden und auf handliche Grösse skalieren\n",
        "\n",
        "image = cv2.imread(\"hands1.jpg\")\n",
        "image = cv2.resize(image, (480, 480))\n",
        "cv2_imshow(image)"
      ],
      "metadata": {
        "id": "-lP12SO0CZ6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hand-Detektor\n",
        "base = python.BaseOptions(model_asset_path=\"hand_landmarker.task\")\n",
        "options = vision.HandLandmarkerOptions(\n",
        "    base_options=base,\n",
        "    num_hands=2\n",
        ")\n",
        "\n",
        "detector = vision.HandLandmarker.create_from_options(options)\n"
      ],
      "metadata": {
        "id": "qADhGn2bCb_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hände erkennen\n",
        "\n",
        "mp_image = mp.Image(image_format=mp.ImageFormat.SRGB,\n",
        "                    data=cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "result = detector.detect(mp_image)\n",
        "\n"
      ],
      "metadata": {
        "id": "XjoYU5jHCzRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ergebnis visualisieren\n",
        "\n",
        "annotated = image.copy()\n",
        "h, w, _ = image.shape\n",
        "\n",
        "for hand in result.hand_landmarks:\n",
        "    for lm in hand:\n",
        "        x = int(lm.x * w)\n",
        "        y = int(lm.y * h)\n",
        "        cv2.circle(annotated, (x, y), 5, (0, 255, 0), -1)\n",
        "\n",
        "cv2_imshow(annotated)\n"
      ],
      "metadata": {
        "id": "RBnAuSXjC8qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aufräumen\n",
        "detector.close()"
      ],
      "metadata": {
        "id": "ZZ4_IotNDEi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLnJuFujNO-I"
      },
      "source": [
        "\n",
        "**MobileNet - SSD**\n",
        "\n",
        "updated: 2026-01-19\n",
        "\n",
        "MobileNet SSD (Single Shot MultiBox Detector) ist ein Modell zur Objekterkennung in Echtzeit. Es erkennt mehrere Objekte in einem Bild und ist für schnelle Verarbeitung ausgelegt.\n",
        "\n",
        "Das Modell kombiniert zwei Ideen:\n",
        "1. MobileNet ist ein leichtes neuronales Netz mit geringem Rechenaufwand.\n",
        "2.  SSD erkennt Objekte und deren Position in einem einzigen Rechenschritt.\n",
        "\n",
        "Als Eingabegroesse wird meist 300 mal 300 Pixel verwendet.\n",
        "Einige Varianten nutzen 512 mal 512 Pixel fuer bessere Genauigkeit, sind aber langsamer.\n",
        "Unabhaengig von der Originalgroesse wird jedes Bild automatisch auf die Modellgroesse skaliert.\n",
        "\n",
        "Typische Einsatzgebiete sind Ueberwachung, Augmented Reality und Robotik.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNgto323NQNR"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/chuanqi305/MobileNet-SSD/  # clone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TphkRBVBNa9f"
      },
      "outputs": [],
      "source": [
        "# Diese beiden Dateien definieren die Netzwerkstruktur und die gelernten Gewichte.\n",
        "proto_file = \"MobileNet-SSD/deploy.prototxt\"\n",
        "model_file = \"MobileNet-SSD/mobilenet_iter_73000.caffemodel\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZEaWuF5Nf6a"
      },
      "outputs": [],
      "source": [
        "# Diese Zeile lädt das MobileNet-SSD-Modell in OpenCV.\n",
        "net = cv2.dnn.readNetFromCaffe(proto_file,model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H79oq_gVNjnc"
      },
      "outputs": [],
      "source": [
        "# Dieses Dictionary ordnet den vom Modell ausgegebenen Klassen-IDs die zugehörigen Objektnamen zu.\n",
        "\n",
        "classNames = { 0: 'background',\n",
        "    1: 'aeroplane', 2: 'bicycle', 3: 'bird', 4: 'boat',\n",
        "    5: 'bottle', 6: 'bus', 7: 'car', 8: 'cat', 9: 'chair',\n",
        "    10: 'cow', 11: 'diningtable', 12: 'dog', 13: 'horse',\n",
        "    14: 'motorbike', 15: 'person', 16: 'pottedplant',\n",
        "    17: 'sheep', 18: 'sofa', 19: 'train', 20: 'tvmonitor' }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnIl_jigNpyo"
      },
      "outputs": [],
      "source": [
        "# Diese Parameter legen fest, wie das Eingabebild für das Modell vorbereitet wird.\n",
        "input_shape=(300, 300)\n",
        "\n",
        "# wir muessen die Bildpixel normalisieren und verwenden dafuer diesen Mittelwert.\n",
        "mean = (127.5,127.5,127.5)\n",
        "\n",
        "# wir skalieren wir das Bild, damit es den Eingabeanforderungen des Modells entspricht\n",
        "scale = 0.007843"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkqfU9LANtCL"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"Dog_Cat.jpg\")\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwPLIzGTNwIQ"
      },
      "outputs": [],
      "source": [
        "# das Bild in das passende Eingabeformat fuer das Netzwerk umgewandet.\n",
        "blob = cv2.dnn.blobFromImage(img, scalefactor=scale, size=input_shape, mean=mean,\n",
        "        swapRB=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPzaK8muOBRH"
      },
      "outputs": [],
      "source": [
        "# das vorbereitete Bild wird an das Netzwerk übergeben und\n",
        "# die Objekterkennungen werden berechnet\n",
        "\n",
        "net.setInput(blob)\n",
        "\n",
        "results = net.forward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI1tPFrzOGiC"
      },
      "outputs": [],
      "source": [
        "# Modellvorhersagen visualisieren:\n",
        "# 1. Erkennungen mit hoher Konfidenz filtern\n",
        "# 2. Objektpositionen im Bild berechen\n",
        "# 3. Bounding Boxen mit Klassenname und Konfidenz anzeigen\n",
        "for i in range(results.shape[2]):\n",
        "\n",
        "    # confidence\n",
        "    confidence = round(results[0, 0, i, 2],2)\n",
        "    if confidence > 0.7:\n",
        "\n",
        "        # class id\n",
        "        id = int(results[0, 0, i, 1])\n",
        "\n",
        "        # 3-6 contains the coordinate\n",
        "        x1, y1, x2, y2 = results[0, 0, i, 3:7]\n",
        "\n",
        "        # print(x1,y1,x2,y2)\n",
        "        # scale these coordinates to out image pixel\n",
        "        ih, iw, ic = img.shape\n",
        "        x1, x2 = int(x1*iw), int(x2*iw)\n",
        "        y1, y2 = int(y1 * ih), int(y2 * ih)\n",
        "        cv2.rectangle(img,\n",
        "                      (x1, y1),\n",
        "                      (x2, y2),\n",
        "                      (0, 255, 0), 2)\n",
        "        cv2.putText(img, f'{classNames[id]}: {confidence:.2f}',\n",
        "                    (x1 + 30, y1 + 30),\n",
        "                    cv2.FONT_HERSHEY_DUPLEX,\n",
        "                    1, (255, 0, 0), 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x39fBCZpOKU9"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4DUwy7dPpLj"
      },
      "source": [
        "**Deep Learning with YOLOv5**\n",
        "\n",
        "updated 2024-01-22\n",
        "\n",
        "YOLOv5 (You Only Look Once, Version 5) ist ein schnelles und genaues Modell zur Echtzeit-Objekterkennung.\n",
        "Es erkennt Objekte in Bildern oder Videos in einem einzigen Verarbeitungsschritt.\n",
        "\n",
        "YOLOv5 laeuft auf CPU und GPU, unterstuetzt Transfer Learning und kann fuer verschiedene Plattformen exportiert werden.\n",
        "Es wird haeufig in Ueberwachung, autonomem Fahren, Industrie und Augmented Reality eingesetzt.\n",
        "\n",
        "Das Modell teilt das Bild in ein Raster und sagt pro Rasterzelle Objektklasse, Position und Sicherheit voraus.\n",
        "Ueberlappende Erkennungen werden durch Non Maximum Suppression zusammengefasst.\n",
        "\n",
        "Neuere YOLO Versionen wie YOLOv6 bis YOLOv8 bauen auf diesen Ideen auf und erweitern sie um zusaetzliche Funktionen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jusoC-AyPp6P"
      },
      "outputs": [],
      "source": [
        "# YOLOv5 wird hier aus dem offiziellen Repository geladen und mit allen benoetigten Abhaengigkeiten installiert.\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt comet_ml  # install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ySp3O8gP5GL"
      },
      "outputs": [],
      "source": [
        "# Torch und Hilfsfunktionen werden geladen und das Notebook fuer die Nutzung von YOLOv5 vorbereitet.\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxa2K2KWQAk5"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images\n",
        "display.Image(filename='runs/detect/exp/bus.jpg', width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Classification with TensorFlow Hub\n",
        "updated: 2026-01-19\n",
        "\n",
        "In diesem Colab testen wir verschiedene Bildklassifikationsmodelle aus TensorFlow Hub.\n",
        "\n",
        "Alle Modelle verwenden ein aehnliches Eingabeformat.\n",
        "Dadurch koennen wir sie leicht austauschen und vergleichen.\n",
        "\n",
        "Ziel ist es, herauszufinden, welches Modell fuer eine bestimmte Aufgabe am besten geeignet ist."
      ],
      "metadata": {
        "id": "4Xo1r6dEMatL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# laedt TensorFlow, TensorFlow Hub und Hilfsbibliotheken\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "bsDvOMxsMmYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "laedt, bereitet Bilder fuer TensorFlow vor und zeigt sie an"
      ],
      "metadata": {
        "id": "_JSOz1YnOiWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# laedt, bereitet Bilder fuer TensorFlow vor und zeigt sie an\n",
        "original_image_cache = {}\n",
        "\n",
        "def preprocess_image(image):\n",
        "  image = np.array(image)\n",
        "  # reshape into shape [batch_size, height, width, num_channels]\n",
        "  img_reshaped = tf.reshape(image, [1, image.shape[0], image.shape[1], image.shape[2]])\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  image = tf.image.convert_image_dtype(img_reshaped, tf.float32)\n",
        "  return image\n",
        "\n",
        "def load_image_from_url(img_url):\n",
        "  \"\"\"Returns an image with shape [1, height, width, num_channels].\"\"\"\n",
        "  user_agent = {'User-agent': 'Colab Sample (https://tensorflow.org)'}\n",
        "  response = requests.get(img_url, headers=user_agent)\n",
        "  image = Image.open(BytesIO(response.content))\n",
        "  image = preprocess_image(image)\n",
        "  return image\n",
        "\n",
        "def load_image(image_url, image_size=256, dynamic_size=False, max_dynamic_size=512):\n",
        "  \"\"\"Loads and preprocesses images.\"\"\"\n",
        "  # Cache image file locally.\n",
        "  if image_url in original_image_cache:\n",
        "    img = original_image_cache[image_url]\n",
        "  elif image_url.startswith('https://'):\n",
        "    img = load_image_from_url(image_url)\n",
        "  else:\n",
        "    fd = tf.io.gfile.GFile(image_url, 'rb')\n",
        "    img = preprocess_image(Image.open(fd))\n",
        "  original_image_cache[image_url] = img\n",
        "  # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\n",
        "  img_raw = img\n",
        "  if tf.reduce_max(img) > 1.0:\n",
        "    img = img / 255.\n",
        "  if len(img.shape) == 3:\n",
        "    img = tf.stack([img, img, img], axis=-1)\n",
        "  if not dynamic_size:\n",
        "    img = tf.image.resize_with_pad(img, image_size, image_size)\n",
        "  elif img.shape[1] > max_dynamic_size or img.shape[2] > max_dynamic_size:\n",
        "    img = tf.image.resize_with_pad(img, max_dynamic_size, max_dynamic_size)\n",
        "  return img, img_raw\n",
        "\n",
        "def show_image(image, title=''):\n",
        "  image_size = image.shape[1]\n",
        "  w = (image_size * 6) // 320\n",
        "  plt.figure(figsize=(w, w))\n",
        "  plt.imshow(image[0], aspect='equal')\n",
        "  plt.axis('off')\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tgPoTJ11M3LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select an Image Classification model\n",
        "image_size = 224\n",
        "dynamic_size = False\n",
        "\n",
        "model_name = \"mobilenet_v2_100_224\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
        "\n",
        "model_handle_map = {\n",
        "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2\",\n",
        "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/classification/2\",\n",
        "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/classification/2\",\n",
        "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/classification/2\",\n",
        "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/classification/2\",\n",
        "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/classification/2\",\n",
        "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/classification/2\",\n",
        "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/classification/2\",\n",
        "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/classification/2\",\n",
        "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/classification/2\",\n",
        "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/classification/2\",\n",
        "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/classification/2\",\n",
        "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/classification/2\",\n",
        "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/classification/2\",\n",
        "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/classification/2\",\n",
        "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/classification/2\",\n",
        "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/classification/2\",\n",
        "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/classification/2\",\n",
        "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/classification/2\",\n",
        "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/classification/2\",\n",
        "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/classification/2\",\n",
        "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/classification/2\",\n",
        "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/classification/2\",\n",
        "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/classification/1\",\n",
        "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/classification/1\",\n",
        "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/classification/1\",\n",
        "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/classification/1\",\n",
        "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/classification/1\",\n",
        "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/classification/1\",\n",
        "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/classification/1\",\n",
        "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/classification/1\",\n",
        "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/ilsvrc2012_classification/1\",\n",
        "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/classification/4\",\n",
        "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/4\",\n",
        "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/classification/4\",\n",
        "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/classification/4\",\n",
        "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/classification/4\",\n",
        "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/classification/4\",\n",
        "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4\",\n",
        "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/classification/4\",\n",
        "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/classification/4\",\n",
        "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/classification/4\",\n",
        "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/classification/4\",\n",
        "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/4\",\n",
        "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4\",\n",
        "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/4\",\n",
        "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5\",\n",
        "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/classification/5\",\n",
        "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5\",\n",
        "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/classification/5\",\n",
        "}\n",
        "\n",
        "model_image_size_map = {\n",
        "  \"efficientnetv2-s\": 384,\n",
        "  \"efficientnetv2-m\": 480,\n",
        "  \"efficientnetv2-l\": 480,\n",
        "  \"efficientnetv2-b0\": 224,\n",
        "  \"efficientnetv2-b1\": 240,\n",
        "  \"efficientnetv2-b2\": 260,\n",
        "  \"efficientnetv2-b3\": 300,\n",
        "  \"efficientnetv2-s-21k\": 384,\n",
        "  \"efficientnetv2-m-21k\": 480,\n",
        "  \"efficientnetv2-l-21k\": 480,\n",
        "  \"efficientnetv2-xl-21k\": 512,\n",
        "  \"efficientnetv2-b0-21k\": 224,\n",
        "  \"efficientnetv2-b1-21k\": 240,\n",
        "  \"efficientnetv2-b2-21k\": 260,\n",
        "  \"efficientnetv2-b3-21k\": 300,\n",
        "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
        "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
        "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
        "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
        "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
        "  \"efficientnetv2-b3-21k-ft1k\": 300,\n",
        "  \"efficientnet_b0\": 224,\n",
        "  \"efficientnet_b1\": 240,\n",
        "  \"efficientnet_b2\": 260,\n",
        "  \"efficientnet_b3\": 300,\n",
        "  \"efficientnet_b4\": 380,\n",
        "  \"efficientnet_b5\": 456,\n",
        "  \"efficientnet_b6\": 528,\n",
        "  \"efficientnet_b7\": 600,\n",
        "  \"inception_v3\": 299,\n",
        "  \"inception_resnet_v2\": 299,\n",
        "  \"mobilenet_v2_100_224\": 224,\n",
        "  \"mobilenet_v2_130_224\": 224,\n",
        "  \"mobilenet_v2_140_224\": 224,\n",
        "  \"nasnet_large\": 331,\n",
        "  \"nasnet_mobile\": 224,\n",
        "  \"pnasnet_large\": 331,\n",
        "  \"resnet_v1_50\": 224,\n",
        "  \"resnet_v1_101\": 224,\n",
        "  \"resnet_v1_152\": 224,\n",
        "  \"resnet_v2_50\": 224,\n",
        "  \"resnet_v2_101\": 224,\n",
        "  \"resnet_v2_152\": 224,\n",
        "  \"mobilenet_v3_small_100_224\": 224,\n",
        "  \"mobilenet_v3_small_075_224\": 224,\n",
        "  \"mobilenet_v3_large_100_224\": 224,\n",
        "  \"mobilenet_v3_large_075_224\": 224,\n",
        "}\n",
        "\n",
        "model_handle = model_handle_map[model_name]\n",
        "\n",
        "print(f\"Selected model: {model_name} : {model_handle}\")\n",
        "\n",
        "\n",
        "max_dynamic_size = 512\n",
        "if model_name in model_image_size_map:\n",
        "  image_size = model_image_size_map[model_name]\n",
        "  dynamic_size = False\n",
        "  print(f\"Images will be converted to {image_size}x{image_size}\")\n",
        "else:\n",
        "  dynamic_size = True\n",
        "  print(f\"Images will be capped to a max size of {max_dynamic_size}x{max_dynamic_size}\")\n",
        "\n",
        "labels_file = \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\"\n",
        "\n",
        "#download labels and creates a maps\n",
        "downloaded_file = tf.keras.utils.get_file(\"labels.txt\", origin=labels_file)\n",
        "\n",
        "classes = []\n",
        "\n",
        "with open(downloaded_file) as f:\n",
        "  labels = f.readlines()\n",
        "  classes = [l.strip() for l in labels]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AZ0sOR4-M_-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# waehlt ein Testbild aus, laedt es und zeigt es skaliert an\n",
        "image_name = \"teapot\" # @param ['turtle', 'tiger', 'bus', 'car', 'cat', 'dog', 'apple', 'banana', 'turtle', 'flamingo', 'piano', 'honeycomb', 'teapot']\n",
        "\n",
        "images_for_test_map = {\n",
        "    \"tiger\": \"https://upload.wikimedia.org/wikipedia/commons/b/b0/Bengal_tiger_%28Panthera_tigris_tigris%29_female_3_crop.jpg\",\n",
        "    \"car\": \"https://upload.wikimedia.org/wikipedia/commons/4/49/2013-2016_Toyota_Corolla_%28ZRE172R%29_SX_sedan_%282018-09-17%29_01.jpg\",\n",
        "    \"cat\": \"https://upload.wikimedia.org/wikipedia/commons/4/4d/Cat_November_2010-1a.jpg\",\n",
        "    \"dog\": \"https://upload.wikimedia.org/wikipedia/commons/archive/a/a9/20090914031557%21Saluki_dog_breed.jpg\",\n",
        "    \"apple\": \"https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg\",\n",
        "    \"banana\": \"https://upload.wikimedia.org/wikipedia/commons/1/1c/Bananas_white_background.jpg\",\n",
        "    \"flamingo\": \"https://upload.wikimedia.org/wikipedia/commons/b/b8/James_Flamingos_MC.jpg\",\n",
        "    \"piano\": \"https://upload.wikimedia.org/wikipedia/commons/d/da/Steinway_%26_Sons_upright_piano%2C_model_K-132%2C_manufactured_at_Steinway%27s_factory_in_Hamburg%2C_Germany.png\",\n",
        "    \"honeycomb\": \"https://upload.wikimedia.org/wikipedia/commons/f/f7/Honey_comb.jpg\",\n",
        "    \"teapot\": \"https://upload.wikimedia.org/wikipedia/commons/4/44/Black_tea_pot_cropped.jpg\",\n",
        "}\n",
        "\n",
        "img_url = images_for_test_map[image_name]\n",
        "image, original_image = load_image(img_url, image_size, dynamic_size, max_dynamic_size)\n",
        "show_image(image, 'Scaled image')"
      ],
      "metadata": {
        "id": "_pf-M8scNZvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# laedt das Klassifikationsmodell und fuehrt einen kurzen Probelauf aus\n",
        "# Probelauf sorgt dafuer, dass das Modell initialisiert und kompiliert wird, bevor wir es wirklich verwenden.\n",
        "classifier = hub.load(model_handle)\n",
        "\n",
        "input_shape = image.shape\n",
        "warmup_input = tf.random.uniform(input_shape, 0, 1.0)\n",
        "warmup_logits = classifier(warmup_input).numpy()"
      ],
      "metadata": {
        "id": "qRlUfuMONkiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fuehrt die Klassifikation aus und zeigt die Top 5 Ergebnisse an\n",
        "probabilities = tf.nn.softmax(classifier(image)).numpy()\n",
        "\n",
        "top_5 = tf.argsort(probabilities, axis=-1, direction=\"DESCENDING\")[0][:5].numpy()\n",
        "np_classes = np.array(classes)\n",
        "\n",
        "# Some models include an additional 'background' class in the predictions, so\n",
        "# we must account for this when reading the class labels.\n",
        "includes_background_class = probabilities.shape[1] == 1001\n",
        "\n",
        "for i, item in enumerate(top_5):\n",
        "  class_index = item if includes_background_class else item + 1\n",
        "  line = f'({i+1}) {class_index:4} - {classes[class_index]}: {probabilities[0][top_5][i]}'\n",
        "  print(line)\n",
        "\n",
        "show_image(image, '')"
      ],
      "metadata": {
        "id": "Wm09ruVWOJya"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}