{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Einführung in Neuronale Netzwerke mit MNIST-Datensatz**\n",
        "\n",
        "**1\tEinführung**\n",
        "\n",
        "Neuronale Netzwerke sind ein zentraler Bestandteil des maschinellen Lernens, insbesondere bei der Bildverarbeitung. Ziel dieses Skripts ist es, ein Modell zur Erkennung handgeschriebener Ziffern auf Basis des MNIST-Datensatzes zu entwickeln. Wir starten mit einem Minimalmodell und erweitern die Komplexität schrittweise."
      ],
      "metadata": {
        "id": "N5ANNI-v6yn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 Installation und Import der Bibliotheken**\n",
        "\n",
        "Zweck der Bibliotheken:\n",
        "NumPy: Für mathematische Operationen.\n",
        "\n",
        "Keras: Zur Erstellung und zum Training neuronaler Netzwerke.\n",
        "\n",
        "Matplotlib: Zur Visualisierung der Daten."
      ],
      "metadata": {
        "id": "hvX0faTc7DQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EdWfA46we7c"
      },
      "outputs": [],
      "source": [
        "# Neuronale Netze mit TensorFlow - Hands-on Script für Google Colab\n",
        "\n",
        "# Installieren von TensorFlow (falls erforderlich)\n",
        "!pip install tensorflow\n",
        "\n",
        "# Importieren der notwendigen Bibliotheken\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MNIST-Daten**\n",
        "\n",
        "Die MNIST-Daten bestehen aus handgeschriebenen Ziffern (0 bis 9). Die Daten sind wie folgt organisiert:\n",
        "\n",
        "    Trainingsdaten:\n",
        "        Anzahl: 60.000 Bilder.\n",
        "        Format: 28x28 Pixel pro Bild, dargestellt in Graustufen (Pixelwerte zwischen 0 und 255).\n",
        "        Labels: Integer-Werte (0–9), die die Ziffer repräsentieren.\n",
        "\n",
        "    Testdaten:\n",
        "        Anzahl: 10.000 Bilder.\n",
        "        Format: Gleich wie die Trainingsdaten."
      ],
      "metadata": {
        "id": "RXwCarZ4g9ND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Das Splitten von Trainingsdaten in Testdaten ist wichtig, um die Modellleistung objektiv zu bewerten und sicherzustellen, dass es auf unbekannte Daten verallgemeinern kann. Testdaten helfen die tatsächliche Leistung des Modells zu messen. Ohne Testdaten könnte ein Modell auf Trainingsdaten gut abschneiden, aber bei neuen Daten versagen. Typischerweise teilt man die Daten im Verhältnis 80/20 oder 90/10 auf, wobei Testdaten ausschließlich zur abschließenden Bewertung genutzt werden."
      ],
      "metadata": {
        "id": "SzGiUADbtqr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Laden und Vorverarbeiten des MNIST-Datensatzes\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisierung der Pixelwerte auf den Bereich [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ],
      "metadata": {
        "id": "lrqj7mrQwqHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zeigen einiger Beispielbilder aus dem Datensatz\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(x_train[i], cmap=\"gray\")\n",
        "    plt.title(f\"Label: {y_train[i]}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "06A3zmAMw7Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Versuch: Einfaches Modell**\n",
        "\n",
        "Einfaches neuronales Netz, das für die Klassifikation des MNIST-Datensatzes konzipiert ist. Es enthält nur eine versteckte Schicht.\n",
        "\n",
        "- Flatten(input_shape=(28, 28)): wandelt die Eingabe von einem 2D-Bild (28x28 Pixel) in einen flachen 1D-Vektor mit 784 Features um.\n",
        "- Dense(64, activation='relu'): eine versteckte Schicht mit 64 Neuronen.\n",
        "- ReLU (Rectified Linear Unit): nichtlineare Aktivierungsfunktion: ReLU(x)=max⁡(0,x). Bringt Nichtlinearität ins Modell, wodurch es komplexere Muster lernen kann.\n",
        "- 64 Neuronen bedeuten, dass diese Schicht 64 gewichtete Summen berechnet, jede basierend auf den 784 Eingabewerten.\n",
        "- Dense(10, activation='softmax'): die Ausgabeschicht hat 10 Neuronen, da MNIST 10 Klassen (Ziffern 0–9) hat.\n",
        "- Softmax-Aktivierung: wandelt die Ausgaben der Schicht in Wahrscheinlichkeiten um, die die Zugehörigkeit zu jeder Klasse beschreiben. Der Wert jedes Neurons liegt zwischen 0 und 1, und die Summe aller Neuronen ist 1.\n",
        "\n",
        "Die Modellzusammenfassung zeigt die Anzahl der Parameter in jeder Schicht:\n",
        "\n",
        "- Flatten-Schicht: Hat keine Parameter, da sie nur die Form der Eingabe ändert.\n",
        "- Dichte Schicht (64 Neuronen). Parameter: 784×64+64=50,240. 784×64: Gewichte für jede Verbindung. +64: Bias-Werte (einer pro Neuron).\n",
        "- Ausgabeschicht (10 Neuronen): Parameter: 64×10+10=650 64×10: Gewichte. +10: Bias-Werte.\n",
        "- Gesamte Parameteranzahl: 50,240+650=50,890\n"
      ],
      "metadata": {
        "id": "tf_Qowi-su9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vereinfachtes neuronales Netz für MNIST\n",
        "\n",
        "# Modellarchitektur: Nur eine versteckte Schicht\n",
        "model1 = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Wandelt 2D-Bild in 1D-Vektor um\n",
        "    Dense(64, activation='relu'),  # Eine versteckte Schicht mit 64 Neuronen\n",
        "    Dense(10, activation='softmax') # Ausgabeschicht für 10 Klassen\n",
        "])\n",
        "\n",
        "# Kompilieren des Modells\n",
        "model1.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Modellübersicht anzeigen\n",
        "model1.summary()\n"
      ],
      "metadata": {
        "id": "XknzyJKoyE-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Trainieren des Modells\n",
        "history1 = model1.fit(x_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# Evaluierung auf den Testdaten\n",
        "test_loss, test_acc = model1.evaluate(x_test, y_test)\n",
        "print(f\"Testgenauigkeit: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "EuZUwf3fv03T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Versuch**\n",
        "\n",
        "Neuronales Netz mit zwei versteckten Schichten, das für Klassifikationsaufgaben wie MNIST optimiert ist\n",
        "\n",
        "Modellarchitektur\n",
        "- Flatten(input_shape=(28, 28)): diese Schicht wandelt die 2D-Bilddaten (28x28 Pixel) in einen flachen 1D-Vektor mit 784 Werten um. Bereitet die Eingabedaten für die vollständig verbundenen Schichten (Dense Layers) vor.\n",
        "- Dense(128, activation='relu'): versteckte Schicht mit 128 Neuronen.\n",
        "- ReLU-Aktivierung: ReLU(x)=max⁡(0,x). ReLU führt Nichtlinearität ein und hilft dem Modell, komplexe Muster zu lernen.\n",
        "- Dense(64, activation='relu'): zweite versteckte Schicht mit 64 Neuronen.   Auch hier wird die ReLU-Aktivierung verwendet, um tiefergehende Merkmale aus den Daten zu extrahieren.\n",
        "- Dense(10, activation='softmax'): Ausgabeschicht mit 10 Neuronen, da das Modell 10 Klassen (Ziffern 0–9) vorhersagen soll.\n",
        "- Softmax-Aktivierung: Gibt Wahrscheinlichkeiten für jede Klasse zurück.       Die Wahrscheinlichkeiten summieren sich zu 1, was für Klassifikationsaufgaben erforderlich ist.\n"
      ],
      "metadata": {
        "id": "WoJvc5tSujgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modellarchitektur definieren\n",
        "model2 = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Wandelt 2D-Bild in 1D-Vektor um\n",
        "    Dense(128, activation='relu'),  # Versteckte Schicht mit 128 Neuronen\n",
        "    Dense(64, activation='relu'),   # Versteckte Schicht mit 64 Neuronen\n",
        "    Dense(10, activation='softmax') # Ausgabeschicht mit 10 Klassen\n",
        "])\n",
        "\n",
        "\n",
        "# Modellübersicht anzeigen\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "gm37pkwhxFjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kompilieren des Modells\n",
        "model2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "mDcaJwc0xNU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainieren des Modells\n",
        "history2 = model2.fit(x_train, y_train, epochs=5, validation_split=0.2)"
      ],
      "metadata": {
        "id": "FFzCauDDxRum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluierung des Modells auf Testdaten\n",
        "test_loss, test_acc = model2.evaluate(x_test, y_test)\n",
        "print(f\"Testgenauigkeit: {test_acc * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "_7v3FsLzxc9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisierung von Trainings- und Validierungsmetriken\n",
        "\n",
        "Die Visualisierung von Trainings- und Validierungsmetriken ist ein entscheidender Schritt, um das Verhalten eines Modells während des Trainings zu analysieren und potenzielle Probleme wie Overfitting oder Unterfitting zu identifizieren.\n",
        "\n",
        "Ideale Kurven:\n",
        "- Der Trainingsverlust sinkt stetig.\n",
        "- Der Validierungsverlust sinkt ebenfalls und bleibt nahe am Trainingsverlust.\n",
        "    \n",
        "Overfitting:\n",
        "- Der Trainingsverlust sinkt weiter, während der Validierungsverlust nach einer bestimmten Epoche wieder ansteigt.\n",
        "- Die Trainingsgenauigkeit steigt weiter, während die Validierungsgenauigkeit stagniert oder sinkt.\n",
        "- Interpretation: Das Modell passt sich zu stark an die Trainingsdaten an und verliert die Fähigkeit zu generalisieren.\n",
        "\n",
        "Underfitting:\n",
        "- Beide Verluste bleiben hoch, und die Genauigkeit stagniert auf niedrigem Niveau.\n",
        "- Interpretation: Das Modell ist nicht komplex genug oder wurde nicht ausreichend trainiert.\n",
        "\n",
        "Schwankende Validierungskurven:\n",
        "- Deuten darauf hin, dass das Modell nicht stabil ist, was oft durch zu große Lernraten oder unzureichendes Training verursacht wird."
      ],
      "metadata": {
        "id": "ach3otmW2O43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisierung von Trainings- und Validierungsmetriken\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Verlustfunktion\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history2.history['loss'], label='Training Loss')\n",
        "plt.plot(history2.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Verlustfunktion')\n",
        "plt.xlabel('Epoche')\n",
        "plt.ylabel('Verlust')\n",
        "plt.legend()\n",
        "\n",
        "# Genauigkeit\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history2.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history2.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Genauigkeit')\n",
        "plt.xlabel('Epoche')\n",
        "plt.ylabel('Genauigkeit')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HH_ABA_WxfHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vorhersage auf neuen Daten\n",
        "predictions = model2.predict(x_test[:5])\n",
        "\n",
        "# Anzeigen der Vorhersagen\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(5):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(x_test[i], cmap=\"gray\")\n",
        "    plt.title(f\"Pred: {predictions[i].argmax()}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KwHwhPtqxjpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Falsche Vorhersagen**"
      ],
      "metadata": {
        "id": "mxSpwvrJwOjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Vorhersagen für die Testdaten\n",
        "predictions = model2.predict(x_test)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Finden der fehlerhaften Vorhersagen\n",
        "incorrect_indices = np.where(predicted_labels != y_test)[0]\n",
        "\n",
        "# Anzahl fehlerhafter Vorhersagen anzeigen\n",
        "print(f\"Anzahl fehlerhafter Vorhersagen: {len(incorrect_indices)}\")\n",
        "\n",
        "# Visualisierung der ersten 10 fehlerhaften Vorhersagen\n",
        "plt.figure(figsize=(15, 8))\n",
        "for i, index in enumerate(incorrect_indices[0:10]):  # Zeigt maximal 10 fehlerhafte Vorhersagen\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(x_test[index], cmap='gray')\n",
        "    plt.title(f\"True: {y_test[index]}, Pred: {predicted_labels[index]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QsIviSGJy0o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Einfaches CNN für MNIST**\n",
        "\n",
        "Convolutional Neural Network (CNN), wurden speziell für Bildverarbeitungsaufgaben entwickelt, z.B. für die Klassifikation des MNIST-Datensatzes.\n",
        "\n",
        "Funktion des Modells\n",
        "- Convolutional Layer: Extrahiert lokale Merkmale (z. B. Kanten, Muster).\n",
        "- Pooling Layer: Reduziert die Größe der Daten und macht die Merkmale translational invariant (robust gegenüber Positionsänderungen).\n",
        "- Dense Layer: Verarbeitet die extrahierten Merkmale und klassifiziert das Bild in eine von 10 Klassen.\n",
        "\n",
        "Typische Parameteranzahl\n",
        "- Conv2D: (3×3×1+1)×32=320 Parameter.\n",
        "- MaxPooling2D: Keine Parameter (da es nur Maximalwerte nimmt).\n",
        "- Flatten: Keine Parameter (ändert nur die Form der Daten).\n",
        "- Dense (128 Neuronen): 6.272×128+128=802.944 Parameter.\n",
        "- Dense (10 Neuronen): 128×10+10=1.290Parameter.\n",
        "\n",
        "Gesamtanzahl der Parameter: 804.554."
      ],
      "metadata": {
        "id": "Y1togeKg0uXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Definition des Modells\n",
        "model3 = Sequential([\n",
        "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Modellübersicht anzeigen\n",
        "model3.summary()"
      ],
      "metadata": {
        "id": "xTQ8Ct-n0rc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history3 = model3.fit(x_train, y_train, epochs=5, validation_split=0.2)"
      ],
      "metadata": {
        "id": "FFRdQm_R2vM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model3.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy (CNN): {test_accuracy}\")"
      ],
      "metadata": {
        "id": "OZ33x0TQ3qqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fashion MNIST**"
      ],
      "metadata": {
        "id": "e54YMgy-4pil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Klassenbezeichnungen für Fashion MNIST\n",
        "class_names = ['T-Shirt/Top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
        "\n"
      ],
      "metadata": {
        "id": "QAy4damL4xGf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}