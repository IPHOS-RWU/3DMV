{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1O_MD9kFcZ5"
      },
      "source": [
        "Vorbereitung: Bilder von Github laden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPqmhUePFNcl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/IPHOS-RWU/3DMV/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ko3wxrFmqx"
      },
      "source": [
        "Vorbereitung: Libraries importieren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pO9s9MNFoaM"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "print(cv2.__version__)\n",
        "%cd 3DMV/images\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpMh6PoxQMaC"
      },
      "source": [
        "1.1 Einstieg in OpenCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmKUKfYoLj-U"
      },
      "outputs": [],
      "source": [
        "print(cv2.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k4GpxMjL8Ad"
      },
      "source": [
        "1.2 Bilder in OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIFa7683MDpQ"
      },
      "outputs": [],
      "source": [
        "airp = cv2.imread(\"airplane.jpg\")\n",
        "print(airp.shape)\n",
        "print(airp.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As8x8VngNKrp"
      },
      "outputs": [],
      "source": [
        "plt.imshow(airp)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1mw-qjnNx0E"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(airp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ToiWOM-OmLR"
      },
      "outputs": [],
      "source": [
        "bab = cv2.imread(\"baboon.jpg\")\n",
        "cv2_imshow(bab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV0gULD3Os9G"
      },
      "outputs": [],
      "source": [
        "bab2 = cv2.cvtColor(bab, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(np.hstack([bab, bab2]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi-CvQiNP8RK"
      },
      "outputs": [],
      "source": [
        "bab_gray = cv2.cvtColor(bab, cv2.COLOR_BGR2GRAY)\n",
        "cv2_imshow(bab_gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_Lm4S90QK7n"
      },
      "outputs": [],
      "source": [
        "plt.imshow(bab_gray, cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwBs86aEQpqp"
      },
      "source": [
        "**1.4 Einfaches Thresholding**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El85NzefQoU_"
      },
      "outputs": [],
      "source": [
        "airp_gray = cv2.cvtColor(airp, cv2.COLOR_BGR2GRAY)\n",
        "mask = cv2.inRange(airp_gray, 128, 255)\n",
        "plt.imshow(mask, \"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J91k4eWaRoXO"
      },
      "outputs": [],
      "source": [
        "print(mask.shape)\n",
        "print(airp_gray.shape)\n",
        "print(airp.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2ZVQoSpSWpH"
      },
      "source": [
        "**1.6 Zeichnen**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K116H8oSZVT"
      },
      "outputs": [],
      "source": [
        "cv2.line(airp, (50,50), (200,200), (255,255,0), 15)\n",
        "cv2.rectangle(airp, (150,100), (200,150), (0,255,0), 5)\n",
        "cv2.circle(airp, (200, 200), 50, (255,0,0), 7)\n",
        "plt.imshow(airp)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OMRd_WgTGZv"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(airp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9OHlMovTgkV"
      },
      "outputs": [],
      "source": [
        "x_values = np.linspace(100,500,50)\n",
        "y_values = np.sin(x_values) * 100 + 200\n",
        "plt.imshow(airp, cmap='gray')\n",
        "plt.plot(x_values,y_values, 'c', linewidth=5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAehIG14T00i"
      },
      "source": [
        "**1.7 Bild speichern**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuTUOd5qT3Ls"
      },
      "outputs": [],
      "source": [
        "cv2.imwrite(\"airplane_new.jpg\", airp)\n",
        "%ls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brDf84UFUx3i"
      },
      "source": [
        "1.8 Auf Pixeldaten zugreifen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynCsGBPEU0XY"
      },
      "outputs": [],
      "source": [
        "# Auf Grauwertbild zugreifen\n",
        "pixel = bab_gray[100,100]\n",
        "print(pixel)\n",
        "# Auf Farbbild zugreifen\n",
        "pixel = bab[100,100]\n",
        "print(pixel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idaiZSXAVs3D"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"baboon.jpg\", cv2.IMREAD_GRAYSCALE )\n",
        "for y in range(20, 70):\n",
        "  for x in range(100, 300):\n",
        "    bab_gray[y+50, x] = bab_gray[y, x]\n",
        "\n",
        "cv2_imshow(bab_gray)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHsZRz4zLZMt"
      },
      "source": [
        "2. Termin 15.10.2024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2UvwYNpSgZ3"
      },
      "source": [
        "**1.2 Laden eines Grauwertbildes und Anzeige des zugehörigen Histogramms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dhl5NUhaLdSQ"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"airplane.jpg\")\n",
        "hist = cv2.calcHist([img], [0], None, [256], [0, 255])\n",
        "plt.figure()\n",
        "plt.plot(hist)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbaOa63fVLLJ"
      },
      "outputs": [],
      "source": [
        "(T, thresh) = cv2.threshold(img, 155, 255, cv2.THRESH_BINARY)\n",
        "plt.imshow(thresh, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYlT6Y7YVXox"
      },
      "outputs": [],
      "source": [
        "(T, thresh) = cv2.threshold(img, 50, 255, cv2.THRESH_BINARY)\n",
        "plt.imshow(thresh, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8pchhpBVyUy"
      },
      "outputs": [],
      "source": [
        "rice = cv2.imread(\"rice.jpg\", cv2.IMREAD_GRAYSCALE )\n",
        "cv2_imshow(rice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-foUbXUWt48"
      },
      "outputs": [],
      "source": [
        "hist = cv2.calcHist([rice], [0], None, [256], [0, 255])\n",
        "plt.figure()\n",
        "plt.plot(hist)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AB6_eW6X2Pi"
      },
      "outputs": [],
      "source": [
        "(T, thresh) = cv2.threshold(rice, 120, 255, cv2.THRESH_BINARY)\n",
        "plt.imshow(thresh, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk1YPofxX9H9"
      },
      "outputs": [],
      "source": [
        "(T, thresh) = cv2.threshold(rice, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "plt.imshow(thresh, \"gray\")\n",
        "print(\"Schwelle nach OTSU=\", T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI7LzTh0aFvB"
      },
      "outputs": [],
      "source": [
        "thresh2 = cv2.adaptiveThreshold(rice ,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,91,2)\n",
        "plt.imshow(thresh2, \"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_Qc6kWvmYki"
      },
      "source": [
        "3. Übung - Addieren und Subtrahieren von Bilder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH5xqqfSmbQb"
      },
      "outputs": [],
      "source": [
        "i1 = cv2.imread(\"patras.jpg\")\n",
        "o1 = cv2.cvtColor(i1, cv2.COLOR_BGR2RGB)\n",
        "print(i1.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFh8mkapncRf"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(i1)\n",
        "# plt.imshow(o1) als Alternative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvtghnpAnzx_"
      },
      "outputs": [],
      "source": [
        "i2 = cv2.imread(\"brycecanyon.jpg\")\n",
        "print(i2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2KD_SiZoAEh"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(i2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfznvPK-oW8c"
      },
      "outputs": [],
      "source": [
        "i1 = i1[0:480, 0:512]\n",
        "i2 = i2[0:480, 0:512]\n",
        "cv2_imshow(np.hstack([i1, i2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ulFXN-7pCYK"
      },
      "outputs": [],
      "source": [
        "z = i1 + i2\n",
        "cv2_imshow(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f6QbgpephZS"
      },
      "outputs": [],
      "source": [
        "i1gray = cv2.cvtColor(i1, cv2.COLOR_BGR2GRAY)\n",
        "i2gray = cv2.cvtColor(i2, cv2.COLOR_BGR2GRAY)\n",
        "zgray = i1gray + i2gray\n",
        "cv2_imshow(zgray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl6bO_MBqYkJ"
      },
      "outputs": [],
      "source": [
        "zgray = 0.5*i1gray + 0.5*i2gray\n",
        "cv2_imshow(zgray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btelJpuxqpPe"
      },
      "outputs": [],
      "source": [
        "z = 0.5*i1 + 0.5*i2\n",
        "cv2_imshow(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsdQS7W7qvLR"
      },
      "outputs": [],
      "source": [
        "z = 0.5 * (i1+i2)\n",
        "cv2_imshow(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOwzgfcOrJrI"
      },
      "source": [
        "Untersuchung und Korrektur des Ergebnisbildes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ewSLcamrJ3n"
      },
      "outputs": [],
      "source": [
        "z = cv2.add(i1, i2)\n",
        "cv2_imshow(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mw8BuJir6-1"
      },
      "source": [
        "Jetzt mit cv2.addWeighted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03Zlbdvcr_lt"
      },
      "outputs": [],
      "source": [
        "z = cv2.addWeighted(i1, 0.3, i2, 0.7, 0)\n",
        "cv2_imshow(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIehn4DosoRO"
      },
      "source": [
        "2. Aufgabe Subtraktion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttSU7aefsrKF"
      },
      "outputs": [],
      "source": [
        "zgray = i2gray - i1gray\n",
        "cv2_imshow(zgray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kS9czSxs61y"
      },
      "outputs": [],
      "source": [
        "zgray = cv2.subtract(i2gray, i1gray)\n",
        "cv2_imshow(zgray)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N_EmlrstdmL"
      },
      "source": [
        "Laden sie das Bild son1.jpg in die Variable A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty913ZehtRZ4"
      },
      "outputs": [],
      "source": [
        "A = cv2.imread(\"son1.jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "print(A.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNsliN83tjDi"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyxIC5UyuGHU"
      },
      "source": [
        "Binarisieren sie das Bild mit dem Befehl threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSLMXMAxuC1M"
      },
      "outputs": [],
      "source": [
        "S, thresh = cv2.threshold(A, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "print(S)\n",
        "cv2_imshow(thresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPHL6dnquIjl"
      },
      "outputs": [],
      "source": [
        "B = cv2.imread(\"son2.jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "cv2_imshow(B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1TlaNkkvj4v"
      },
      "outputs": [],
      "source": [
        "x = A + 90 - B\n",
        "S, thresh = cv2.threshold(x, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "print(S)\n",
        "cv2_imshow(thresh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB5jHLupw9tx"
      },
      "source": [
        "**4. Übung - Lineare Filter**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzHAdJrUxOVX"
      },
      "source": [
        "2 Mittelwertsfilter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUmquwVowbrJ"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"baboon.jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "print(img.shape)\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V89v5h-EymL3"
      },
      "outputs": [],
      "source": [
        "kernel = np.ones( (51, 51) ) / 51**2\n",
        "print(kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EowFUPlAzKT6"
      },
      "outputs": [],
      "source": [
        "dst = cv2.filter2D(img, -1, kernel)\n",
        "cv2_imshow(np.hstack([img, dst]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiBwGBmI0Yw8"
      },
      "outputs": [],
      "source": [
        "dst = cv2.boxFilter(img, -1, (21, 21))\n",
        "cv2_imshow(dst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-YM6mDg0h-h"
      },
      "outputs": [],
      "source": [
        "dst = np.hstack([cv2.boxFilter(img, -1, (3, 3)),\n",
        "cv2.boxFilter(img, -1, (15, 15)), cv2.boxFilter(img, -1,\n",
        "(31, 31))])\n",
        "cv2_imshow(dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFkI97OrnoNs"
      },
      "source": [
        "**3 Gauss Filter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzsD-miKozrQ"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"baboon.jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0UzhhOBn2Wj"
      },
      "outputs": [],
      "source": [
        "kernel = np.matrix( ((0,1,2,1,0), (1,3,5,3,1), (2,5,9,5,2), (1,3,5,3,1), (0,1,2,1,0))) / 57\n",
        "print(kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G624iYvGo6QV"
      },
      "outputs": [],
      "source": [
        "dst = cv2.filter2D(img, -1, kernel)\n",
        "cv2_imshow(np.hstack([img, dst]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWZugk5mpnzC"
      },
      "outputs": [],
      "source": [
        "dst = cv2.GaussianBlur(img, (5, 5), 3)\n",
        "cv2_imshow(np.hstack([img, dst]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd9pHPKHp5Ma"
      },
      "source": [
        "**4 Median Filterung**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgJh8Zl4q0q2"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"lena_noise.jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P7OBEjptxvz"
      },
      "outputs": [],
      "source": [
        "dst = cv2.medianBlur(img, 5)\n",
        "cv2_imshow(np.hstack([img, dst]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdEHODfevDNU"
      },
      "source": [
        "8 Aufgabe Unscharfmaskierung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw1THcVIvGAG"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"monkey.jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql8vr8YIv5Kc"
      },
      "outputs": [],
      "source": [
        "lp = cv2.GaussianBlur(img, (11, 11), 3)\n",
        "hp = cv2.subtract(img, lp)\n",
        "img2 = cv2.add(img, hp)\n",
        "cv2_imshow(np.hstack([img, img2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liAoh6sUjwta"
      },
      "source": [
        "**5 Kanten / Gradienten-Filter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2KyJX52j18n"
      },
      "outputs": [],
      "source": [
        "kernel = np.matrix( ((1, 0, -1), (2, 0, -2), (1, 0, -1)) )\n",
        "print(kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ7Ahifxo79m"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('zebra.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "dst = cv2.filter2D(img, -1, kernel)\n",
        "cv2_imshow(np.hstack([img, dst]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QI6xYpnqMHQ"
      },
      "outputs": [],
      "source": [
        "kernel_h = np.transpose(kernel)\n",
        "print(kernel_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRLFf3dXq63V"
      },
      "outputs": [],
      "source": [
        "dst2 = cv2.filter2D(img, -1, kernel_h)\n",
        "cv2_imshow(np.hstack([dst, dst2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfsJgQTcraBX"
      },
      "outputs": [],
      "source": [
        "dst3 = cv2.add(dst, dst2)\n",
        "cv2_imshow(np.hstack([img, dst3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSBlvFxPrxIo"
      },
      "outputs": [],
      "source": [
        "dst5 = cv2.Sobel(img, -1, 0, 1)\n",
        "cv2_imshow(np.hstack([img, dst5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbwlaUr9t76i"
      },
      "source": [
        "**6 Laplacian Filterung**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23hYntnms_yC"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('zebra.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "dst = cv2.Laplacian(img, -1, 8, scale=4)\n",
        "cv2_imshow(np.hstack([img, dst]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-ZU96f7vrgh"
      },
      "source": [
        "**7 Canny Edge Detection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE5-nin9vwDC"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('sudoku.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "dst = cv2.Canny(img, 100, 200)\n",
        "cv2_imshow(np.hstack([img, dst]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfke3DDCxXL3"
      },
      "source": [
        "**5. Übung - Objekte Segmentieren**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMPvNLb_xcH-"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('coins.jpg')\n",
        "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "cv2_imshow(img_gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xUA63Uky1Zz"
      },
      "outputs": [],
      "source": [
        "lap = cv2.Laplacian(img_gray, cv2.CV_64F)\n",
        "lap = np.uint8(np.absolute(lap))\n",
        "cv2_imshow(np.hstack([img_gray, lap]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0lQERl_03m9"
      },
      "outputs": [],
      "source": [
        "sobX = cv2.Sobel(img_gray, cv2.CV_64F, 1, 0)\n",
        "sobY = cv2.Sobel(img_gray, cv2.CV_64F, 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaTvDvnQ0xTK"
      },
      "outputs": [],
      "source": [
        "sobC = np.sqrt(sobX**2 + sobY**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a8uTpVO1GMx"
      },
      "outputs": [],
      "source": [
        "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "sobX = np.uint8(np.absolute(sobX))\n",
        "sobY = np.uint8(np.absolute(sobY))\n",
        "sobC = np.uint8(np.absolute(sobC))\n",
        "plt.subplot(2, 2, 1), plt.imshow(img_rgb, \"gray\")\n",
        "plt.subplot(2, 2, 2), plt.imshow(sobX, \"gray\")\n",
        "plt.subplot(2, 2, 3), plt.imshow(sobY, \"gray\")\n",
        "plt.subplot(2, 2, 4), plt.imshow(sobC, \"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JECsbjM2Pav"
      },
      "source": [
        "**2 Canny-Edge-Operator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaSPPX_42UUp"
      },
      "outputs": [],
      "source": [
        "canny = cv2.Canny(img_gray, 30, 150)\n",
        "cv2_imshow(np.hstack([img_gray, canny]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gr8P62A4bju"
      },
      "outputs": [],
      "source": [
        "img_gauss = cv2.GaussianBlur(img_gray, (5, 5), 0)\n",
        "canny = cv2.Canny(img_gauss, 30, 150)\n",
        "cv2_imshow(np.hstack([img_gray, canny]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBhLFl4W5bK3"
      },
      "source": [
        "Optimieren nach Kontur\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvX7QJFd5dUD"
      },
      "outputs": [],
      "source": [
        "img_gauss = cv2.GaussianBlur(img_gray, (7, 7), 0)\n",
        "canny = cv2.Canny(img_gauss, 120, 200)\n",
        "cv2_imshow(np.hstack([img_gray, canny]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_cHZ7ptvrL-"
      },
      "outputs": [],
      "source": [
        "contours, hierarchy = cv2.findContours(canny, cv2.RETR_EXTERNAL,\n",
        "cv2.CHAIN_APPROX_SIMPLE)\n",
        "print(\"Anzahl der Münzen im Bild=\", len(contours))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZvlVDYowSIl"
      },
      "outputs": [],
      "source": [
        "coins = img.copy()\n",
        "cv2.drawContours(coins, contours, -1, (0, 255, 0), 4)\n",
        "cv2_imshow(np.hstack([img, coins]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRRrsgnvyJGn"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"smartie123.jpg\")\n",
        "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "img_gauss = cv2.GaussianBlur(img_gray, (7, 7), 0)\n",
        "canny = cv2.Canny(img_gauss, 130, 170)\n",
        "contours, hierarchy = cv2.findContours(canny, cv2.RETR_EXTERNAL,\n",
        "cv2.CHAIN_APPROX_SIMPLE)\n",
        "print(\"Anzahl der Smarties im Bild=\", len(contours))\n",
        "\n",
        "smartie = img.copy()\n",
        "cv2.drawContours(smartie, contours, -1, (0, 255, 0), 4)\n",
        "cv2_imshow(np.hstack([img, smartie]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qow1TZAd2RsU"
      },
      "source": [
        "3.3 Aufgabe: Konturen finden mit dem threshold Operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOeQwjhD2VRo"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"shapes.jpg\")\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjrnO9sM23lL"
      },
      "outputs": [],
      "source": [
        "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "img_blur = cv2.GaussianBlur(img_gray, (5, 5), 0)\n",
        "S, thresh = cv2.threshold(img_blur, 70 ,255, cv2.THRESH_BINARY)\n",
        "cv2_imshow(thresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-wIhTPX3nUT"
      },
      "outputs": [],
      "source": [
        "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL,\n",
        "cv2.CHAIN_APPROX_SIMPLE)\n",
        "print(\"Anzahl der Objekte im Bild=\", len(contours))\n",
        "\n",
        "bild = img.copy()\n",
        "cv2.drawContours(bild, contours, -1, (0, 255, 0), 4)\n",
        "cv2_imshow(np.hstack([img, bild]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQGvYN5d4kNZ"
      },
      "outputs": [],
      "source": [
        "for c in contours:\n",
        "  # compute the center of the contour\n",
        "  M = cv2.moments(c)\n",
        "  A = cv2.contourArea(c)\n",
        "  if M[\"m00\"]==0:\n",
        "    continue\n",
        "  cX = int(M[\"m10\"] / (M[\"m00\"]))\n",
        "  cY = int(M[\"m01\"] / (M[\"m00\"]))\n",
        "  # draw the contour and center of the shape on the image\n",
        "  cv2.drawContours(bild, [c], -1, (0, 255, 0), 2)\n",
        "  cv2.circle(bild, (cX, cY), 7, (255, 255, 255), -1)\n",
        "  cv2.putText(bild, str(A), (cX - 20, cY - 20),\n",
        "  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,\n",
        "  255, 255), 2)\n",
        "\n",
        "cv2_imshow(bild)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-kcWxDs6wbo"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"smartie123.jpg\")\n",
        "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "img_blur = cv2.GaussianBlur(img_gray, (5, 5), 0)\n",
        "S, thresh = cv2.threshold(img_blur, 70 ,255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Define the kernel for erosion\n",
        "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))  # Adjust size as needed\n",
        "\n",
        "# Apply erosion\n",
        "eroded = cv2.erode(thresh, kernel, iterations=5)\n",
        "\n",
        "cv2_imshow(eroded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU06Dkud7P_u"
      },
      "outputs": [],
      "source": [
        "contours, hierarchy = cv2.findContours(eroded, cv2.RETR_EXTERNAL,\n",
        "cv2.CHAIN_APPROX_SIMPLE)\n",
        "print(\"Anzahl der Objekte im Bild=\", len(contours))\n",
        "\n",
        "bild = img.copy()\n",
        "\n",
        "for c in contours:\n",
        "  # compute the center of the contour\n",
        "  M = cv2.moments(c)\n",
        "  A = cv2.contourArea(c)\n",
        "  if M[\"m00\"]==0:\n",
        "    continue\n",
        "  cX = int(M[\"m10\"] / (M[\"m00\"]))\n",
        "  cY = int(M[\"m01\"] / (M[\"m00\"]))\n",
        "  # draw the contour and center of the shape on the image\n",
        "  cv2.drawContours(bild, [c], -1, (0, 255, 0), 2)\n",
        "  cv2.circle(bild, (cX, cY), 7, (255, 255, 255), -1)\n",
        "  cv2.putText(bild, str(A), (cX - 20, cY - 20),\n",
        "  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,\n",
        "  255, 255), 2)\n",
        "\n",
        "cv2_imshow(bild)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKHGqe6ywfkC"
      },
      "source": [
        "**4. Übung - RGB und HSV Color Space**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkN2qR-TwleT"
      },
      "outputs": [],
      "source": [
        "indoor = cv2.imread('cube_indoor.jpg')\n",
        "outdoor= cv2.imread('cube_outdoor.jpg')\n",
        "cv2_imshow(np.hstack([indoor, outdoor]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ5o15Zrxio3"
      },
      "outputs": [],
      "source": [
        "i_blue, i_red, i_green = cv2.split(indoor)\n",
        "cv2_imshow(np.hstack([i_blue, i_green, i_red]))\n",
        "\n",
        "o_blue, o_red, o_green = cv2.split(outdoor)\n",
        "cv2_imshow(np.hstack([o_blue, o_green, o_red]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAmRaOV6zAlx"
      },
      "outputs": [],
      "source": [
        "indoor_HSV = cv2.cvtColor(indoor, cv2.COLOR_BGR2HSV)\n",
        "outdoor_HSV = cv2.cvtColor(outdoor, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "i_hue, i_sat, i_value = cv2.split(indoor_HSV)\n",
        "cv2_imshow(np.hstack([i_hue, i_sat, i_value]))\n",
        "\n",
        "o_hue, o_sat, o_value = cv2.split(outdoor_HSV)\n",
        "cv2_imshow(np.hstack([o_hue, o_sat, o_value]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQF5yAjo2KmF"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('chalk.jpg')\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIUd4OTx25I_"
      },
      "outputs": [],
      "source": [
        "hsv = cv2.cvtColor(img, cv2.coin = image[170:220, 75:130])\n",
        "\n",
        "lower = (25, 30, 30)\n",
        "upper = (35, 250, 250)\n",
        "\n",
        "mask  = cv2.inRange(hsv, lower, upper)\n",
        "cv2_imshow(mask)\n",
        "img2  = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "cv2_imshow(np.hstack([img, img2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iz-o4A44WT9"
      },
      "source": [
        "Probien mit magenta-farbener Kreide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIpLj8y64Z3_"
      },
      "outputs": [],
      "source": [
        "h_value = 150\n",
        "lower = (h_value, 30, 30)\n",
        "upper = (h_value+20, 250, 250)\n",
        "\n",
        "mask  = cv2.inRange(hsv, lower, upper)\n",
        "img2  = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "cv2_imshow(np.hstack([img, img2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrtDLEtQ6Ju3"
      },
      "outputs": [],
      "source": [
        "def myFunc(x):\n",
        "  lower = (x, 10, 10)\n",
        "  upper = (x+10, 255, 255)\n",
        "  mask = cv2.inRange(hsv, lower, upper)\n",
        "  img2 = cv2.bitwise_and(img, img, mask=mask)\n",
        "  cv2_imshow(np.hstack([img,img2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-KlKRDQ6YE8"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import interact\n",
        "interact(myFunc, x=(0.0,180.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kts-zFCS7tXU"
      },
      "outputs": [],
      "source": [
        "bgr = cv2.split(img)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(131)\n",
        "hist = cv2.calcHist([bgr[1], bgr[0]], [0, 1], None, [32, 32], [0, 256, 0, 256])\n",
        "p = ax.imshow(hist, interpolation = \"nearest\")\n",
        "ax.set_title(\"2DHist G-B\")\n",
        "# plot a 2D color histogram for green and red\n",
        "ax = fig.add_subplot(132)\n",
        "hist = cv2.calcHist([bgr[1], bgr[2]], [0, 1], None, [32, 32], [0, 256, 0, 256])\n",
        "p = ax.imshow(hist, interpolation = \"nearest\")\n",
        "ax.set_title(\"2DHist G-R\")\n",
        "# plot a 2D color histogram for blue and red\n",
        "ax = fig.add_subplot(133)\n",
        "hist = cv2.calcHist([bgr[0], bgr[2]], [0, 1], None, [32, 32], [0, 256, 0,\n",
        "256])\n",
        "p = ax.imshow(hist, interpolation = \"nearest\")\n",
        "ax.set_title(\"2DHist B-R\")\n",
        "# examine the dimensionality of one of the 2D histograms\n",
        "print(\"2D histogram shape: %s, with %d values\" % (hist.shape,\n",
        "hist.flatten().shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyTnmyst8dG-"
      },
      "source": [
        "**6. Übung - Image Processing with scikit-image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAAdz0s38cuZ"
      },
      "outputs": [],
      "source": [
        "import skimage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O_BFPdz9NA3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from skimage import data\n",
        "camera = data.camera()\n",
        "print(type(camera))\n",
        "print(camera.shape)\n",
        "\n",
        "plt.imshow(camera, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0i3sTHw-Fm8"
      },
      "outputs": [],
      "source": [
        "moon = data.moon()\n",
        "plt.imshow(moon, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcpwTaak_T_g"
      },
      "source": [
        "1.3 Image Inversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-xmFd-7_SI6"
      },
      "outputs": [],
      "source": [
        "camera_inv = skimage.util.invert(camera)\n",
        "plt.imshow(camera_inv, 'gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKyyxR6iafbr"
      },
      "outputs": [],
      "source": [
        "camera_inv = cv2.bitwise_not(camera)  # Invert the image\n",
        "cv2_imshow(camera_inv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI-DRudyAKMx"
      },
      "source": [
        "2. Color Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN2TueMDAMNp"
      },
      "outputs": [],
      "source": [
        "img = skimage.data.coffee()\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfdVdigOAUiW"
      },
      "outputs": [],
      "source": [
        "img_gray = skimage.color.rgb2gray(img)\n",
        "plt.imshow(img_gray, 'gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfiXVF2I_dqb"
      },
      "source": [
        "2.1 Conversion between color models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWzS0MLJA0eB"
      },
      "outputs": [],
      "source": [
        "hsv_img = skimage.color.rgb2hsv(img)\n",
        "\n",
        "hue = hsv_img[:, :, 0]\n",
        "sat = hsv_img[:, :, 1]\n",
        "val = hsv_img[:, :, 2]\n",
        "\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(8, 2))\n",
        "ax0.imshow(img)\n",
        "ax1.imshow(hue)\n",
        "ax2.imshow(val, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phK3k_aja066"
      },
      "outputs": [],
      "source": [
        "hsv_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
        "\n",
        "# Split the HSV channels\n",
        "hue, sat, val = cv2.split(hsv_img)\n",
        "\n",
        "# Plot the original image and HSV channels\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(8, 2))\n",
        "ax0.imshow(img)          # Original image in RGB\n",
        "ax0.set_title('Original Image')\n",
        "ax0.axis('off')\n",
        "\n",
        "ax1.imshow(hue)  # Hue channel (cmap='hsv' gives intuitive color representation)\n",
        "ax1.set_title('Hue')\n",
        "ax1.axis('off')\n",
        "\n",
        "ax2.imshow(val, cmap='gray') # Value channel (grayscale)\n",
        "ax2.set_title('Value')\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lorKIfDtwRzA"
      },
      "source": [
        "3 Rescale and Resize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQcDGsCgwT0c"
      },
      "outputs": [],
      "source": [
        "img = skimage.color.rgb2gray(skimage.data.coffee())\n",
        "print(img.shape)\n",
        "img_rescaled = skimage.transform.rescale(img, 0.1)\n",
        "img_resize = skimage.transform.resize(img, (img.shape[0] // 3, img.shape[1] // 2))\n",
        "\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(8, 2))\n",
        "ax0.imshow(img, \"gray\")\n",
        "ax1.imshow(img_rescaled, \"gray\")\n",
        "ax2.imshow(img_resize, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKcEufT5bQ6Y"
      },
      "outputs": [],
      "source": [
        "# Rescale the image (scaling by 0.1)\n",
        "scale_factor = 0.1\n",
        "img_rescaled = cv2.resize(img, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "# Resize the image (specific dimensions)\n",
        "new_height = img.shape[0] // 3\n",
        "new_width = img.shape[1] // 2\n",
        "img_resize = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "# Plot the original, rescaled, and resized images\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(8, 2))\n",
        "ax0.imshow(img, cmap=\"gray\")       # Original image\n",
        "ax0.set_title(\"Original\")\n",
        "ax0.axis(\"off\")\n",
        "\n",
        "ax1.imshow(img_rescaled, cmap=\"gray\") # Rescaled image\n",
        "ax1.set_title(\"Rescaled\")\n",
        "ax1.axis(\"off\")\n",
        "\n",
        "ax2.imshow(img_resize, cmap=\"gray\")   # Resized image\n",
        "ax2.set_title(\"Resized\")\n",
        "ax2.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O3GpwZE0kIc"
      },
      "source": [
        "4 Histogram Equalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE9wCOD60nAk"
      },
      "outputs": [],
      "source": [
        "img = skimage.data.moon()\n",
        "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(8, 2))\n",
        "ax0.imshow(img, \"gray\")\n",
        "ax1.hist(img.ravel(), bins=256, range=[0, 256], color='gray', histtype='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yOFMEXrb_Zj"
      },
      "outputs": [],
      "source": [
        "img.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySiy1szW1d-D"
      },
      "outputs": [],
      "source": [
        "img_eq = skimage.exposure.equalize_hist(img)\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(8, 2))\n",
        "ax0.imshow(img, \"gray\")\n",
        "ax1.imshow(img_eq, \"gray\")\n",
        "ax2.hist(img_eq.ravel(), bins=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HsK50WGcajp"
      },
      "outputs": [],
      "source": [
        "# Apply histogram equalization\n",
        "img_eq = cv2.equalizeHist(img)\n",
        "\n",
        "# Create a figure with three subplots\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(8, 2))\n",
        "\n",
        "# Original image\n",
        "ax0.imshow(img, cmap=\"gray\")\n",
        "ax0.set_title(\"Original Image\")\n",
        "ax0.axis(\"off\")\n",
        "\n",
        "# Equalized image\n",
        "ax1.imshow(img_eq, cmap=\"gray\")\n",
        "ax1.set_title(\"Equalized Image\")\n",
        "ax1.axis(\"off\")\n",
        "\n",
        "# Histogram of the equalized image\n",
        "ax2.hist(img_eq.ravel(), bins=256, range=[0, 256], color='gray', histtype='bar')\n",
        "ax2.set_title(\"Histogram\")\n",
        "ax2.set_xlabel(\"Pixel Intensity\")\n",
        "ax2.set_ylabel(\"Frequency\")\n",
        "\n",
        "# Display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiVhlDA72ll7"
      },
      "outputs": [],
      "source": [
        "# Contrast stretching\n",
        "p2, p98 = np.percentile(img, (5, 95))\n",
        "img_rescale = skimage.exposure.rescale_intensity(img, in_range=(p2, p98))\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(8, 2))\n",
        "ax0.imshow(img, \"gray\")\n",
        "ax1.imshow(img_rescale, \"gray\")\n",
        "ax2.hist(img_rescale.ravel(), bins=256)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la71Dhg_3Yti"
      },
      "source": [
        "5 Thresholding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXZyqAVz3PrV"
      },
      "outputs": [],
      "source": [
        "img = skimage.data.camera()\n",
        "thresh = skimage.filters.threshold_otsu(img)\n",
        "print(thresh)\n",
        "binary = img > thresh\n",
        "plt.imshow(binary, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4gpvOCXqqaX"
      },
      "outputs": [],
      "source": [
        "# Compute Otsu's threshold\n",
        "thresh, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "print(\"Otsu's Threshold=\", thresh)\n",
        "\n",
        "# Display the binary image\n",
        "cv2_imshow(binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaLVJ4tV37Bm"
      },
      "outputs": [],
      "source": [
        "thresh = skimage.filters.threshold_otsu(img)\n",
        "binary = img > thresh\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(8, 2))\n",
        "ax0.imshow(img, \"gray\")\n",
        "ax1.hist(img.ravel(), bins=256)\n",
        "ax1.axvline(thresh, color='g')\n",
        "ax2.imshow(binary, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgNViNRh4hd4"
      },
      "outputs": [],
      "source": [
        "img = skimage.data.page()\n",
        "plt.imshow(img, \"gray\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br6EQ9QV5EdI"
      },
      "outputs": [],
      "source": [
        "thresh = skimage.filters.threshold_otsu(img)\n",
        "binary = img > thresh\n",
        "plt.imshow(binary, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvQMArj75RIX"
      },
      "outputs": [],
      "source": [
        "fig, ax = skimage.filters.try_all_threshold(img, figsize=(8, 6))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe6I3rbU5t5Q"
      },
      "source": [
        "6 Edge Operators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtgNle8w5wNo"
      },
      "outputs": [],
      "source": [
        "img = skimage.data.camera()\n",
        "edge_sobel = skimage.filters.sobel(img)\n",
        "fig, (ax0, ax1) = plt.subplots(ncols=2)\n",
        "ax0.imshow(img, \"gray\")\n",
        "ax1.imshow(edge_sobel, \"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDuERUZK63ar"
      },
      "source": [
        "**7. Übung - Adaptive binarization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW-0gr6z66eL"
      },
      "outputs": [],
      "source": [
        "img = skimage.data.page()\n",
        "plt.imshow(img, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJX39I5n8Tr7"
      },
      "outputs": [],
      "source": [
        "thresh_niblack = skimage.filters.threshold_niblack(img, window_size=25, k=1.0)\n",
        "binary_niblack = img > thresh_niblack\n",
        "plt.imshow(binary_niblack, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brgr4uid9fmQ"
      },
      "outputs": [],
      "source": [
        "thresh_sauvola = skimage.filters.threshold_sauvola(img, window_size=25)\n",
        "binary_sauvola = img > thresh_sauvola\n",
        "plt.imshow(binary_sauvola, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyMHhieC8-io"
      },
      "outputs": [],
      "source": [
        "rice = skimage.io.imread('rice.jpg')\n",
        "plt.imshow(rice, \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z2IBmFK-h4H"
      },
      "outputs": [],
      "source": [
        "thresh_niblack = skimage.filters.threshold_niblack(rice, window_size=59, k=0.01)\n",
        "binary_niblack = rice > thresh_niblack\n",
        "plt.imshow(binary_niblack, \"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcLQdIzj_06U"
      },
      "source": [
        "Multi-Otsu threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEiQJf_I_28y"
      },
      "outputs": [],
      "source": [
        "image = skimage.data.camera()\n",
        "# Applying multi-Otsu threshold for the default value, generating\n",
        "# three classes.\n",
        "thresholds = skimage.filters.threshold_multiotsu(image)\n",
        "# Using the threshold values, we generate the three regions.\n",
        "regions = np.digitize(image, bins=thresholds)\n",
        "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 3.5))\n",
        "# Plotting the original image.\n",
        "ax[0].imshow(image, cmap='gray')\n",
        "ax[0].set_title('Original')\n",
        "ax[0].axis('off')\n",
        "# Plotting the histogram and the two thresholds obtained from\n",
        "# multi-Otsu.\n",
        "ax[1].hist(image.ravel(), bins=255)\n",
        "ax[1].set_title('Histogram')\n",
        "for thresh in thresholds:\n",
        "  ax[1].axvline(thresh, color='r')\n",
        "# Plotting the Multi Otsu result.\n",
        "ax[2].imshow(regions, cmap='jet')\n",
        "ax[2].set_title('Multi-Otsu result')\n",
        "ax[2].axis('off')\n",
        "plt.subplots_adjust()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CmqMqKx_4iT"
      },
      "source": [
        "**Übung 8 - Skeleton**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs5jD1JO_-FF"
      },
      "outputs": [],
      "source": [
        "import skimage\n",
        "\n",
        "horse = skimage.data.horse()\n",
        "\n",
        "image = skimage.util.invert(horse)\n",
        "skeleton = skimage.morphology.skeletonize(image)\n",
        "\n",
        "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
        "ax0.imshow(image, 'gray')\n",
        "ax1.imshow(skeleton, 'gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NwZgxaeB0WG"
      },
      "source": [
        "**9. Übung - Template Matching**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2sEtipCB5AH"
      },
      "outputs": [],
      "source": [
        "image = skimage.data.coins()\n",
        "\n",
        "coin = image[170:220, 75:130]\n",
        "\n",
        "plt.imshow(coin, \"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDP3dNLWDPTg"
      },
      "outputs": [],
      "source": [
        "result = skimage.feature.match_template(image, coin)\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(8,2))\n",
        "ax1.imshow(coin, 'gray')\n",
        "ax2.imshow(image, 'gray')\n",
        "ax3.imshow(result)\n",
        "# highlight matched region\n",
        "yx = np.unravel_index(np.argmax(result), result.shape)\n",
        "ax3.plot(yx[1], yx[0], 'o', markeredgecolor='r',\n",
        "markerfacecolor='none', markersize=10)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIZzcw7jD9Jc"
      },
      "source": [
        "Übung: WaldoBeach mit OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVVURXl-D_kd"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('woistwaldo.jpg')\n",
        "print(img.shape)\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehil3R45LtWP"
      },
      "outputs": [],
      "source": [
        "template = cv2.imread(\"waldo.jpg\")\n",
        "w, h = template.shape[:2]\n",
        "print(w, h)\n",
        "cv2_imshow(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "huJkbL9uFvL9"
      },
      "outputs": [],
      "source": [
        "# Template-Matching durchführen\n",
        "result = cv2.matchTemplate(img, template, cv2.TM_CCOEFF_NORMED)\n",
        "\n",
        "# Maximalen Übereinstimmungspunkt finden\n",
        "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
        "\n",
        "# Koordinaten für das Rechteck um Waldo\n",
        "start_point = max_loc\n",
        "end_point = (start_point[0] + template.shape[1], start_point[1] + template.shape[0])\n",
        "\n",
        "# Rechteck zeichnen\n",
        "cv2.rectangle(img, start_point, end_point, (0, 0, 255), 3)\n",
        "\n",
        "cv2_imshow(img)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mns6SIZi-JRY"
      },
      "source": [
        "Face Detector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-KzZfVk-MAl"
      },
      "outputs": [],
      "source": [
        "nadia = cv2.imread('Nadia_Murad.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "denis = cv2.imread('Denis_Mukwege.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "solvay = cv2.imread('solvay_conference.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "print(nadia.shape)\n",
        "print(denis.shape)\n",
        "print(solvay.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbDH5x5q-fOQ"
      },
      "outputs": [],
      "source": [
        "plt.imshow(nadia,cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srI4bBXb-t3w"
      },
      "source": [
        "**Cascade Files in OpenCV: Gesichtserkennung**\n",
        "\n",
        "Cascade Files sind vortrainierte Modelle in OpenCV, die zur Objekterkennung, insbesondere zur Gesichtserkennung, verwendet werden. Sie basieren auf dem Haar-like Feature-basierten Cascade Classifier, einer Methode, die von Viola und Jones entwickelt und 2001 veröffentlicht wurde. Die Modelle sind als XML-Dateien gespeichert, z. B.:\n",
        "- haarcascade_frontalface_default.xml (Frontalgesichter)\n",
        "- haarcascade_eye.xml (Augen)\n",
        "- haarcascade_smile.xml (Lächeln)\n",
        "\n",
        "**Funktionsweise**\n",
        "\n",
        "Haar-like Features: Erfassen grundlegende Muster wie Kanten und Linien.\n",
        "Integralbild: Beschleunigt die Berechnung von Features.\n",
        "Cascade of Classifiers: Führt eine schrittweise Filterung von Bildregionen durch, um potenzielle Gesichter zu erkennen.\n",
        "Sliding Window: Überprüft das Bild in verschiedenen Größen und Positionen.\n",
        "1. Haar-like Features: Erfassen grundlegende Muster wie Kanten und Linien.\n",
        "2. Integralbild: Beschleunigt die Berechnung von Features.\n",
        "3. Cascade of Classifiers: Führt eine schrittweise Filterung von Bildregionen durch, um potenzielle Gesichter zu erkennen.\n",
        "4. Sliding Window: Überprüft das Bild in verschiedenen Größen und Positionen.\n",
        "\n",
        "**Vorteile**:\n",
        "- Schnell und ressourcenschonend, ideal für Echtzeitanwendungen.\n",
        "- Einfach in der Implementierung mit vortrainierten Modellen.\n",
        "\n",
        "**Nachteile**:\n",
        "- Empfindlich gegenüber Beleuchtung und Hintergrundvariationen.\n",
        "- Veraltet im Vergleich zu modernen Deep-Learning-Methoden wie CNNs.\n",
        "\n",
        "**Weiterentwicklungen**\n",
        "\n",
        "Neuere Ansätze wie Convolutional Neural Networks (z. B. MTCNN oder Dlib) und das DNN-Modul in OpenCV bieten bessere Genauigkeit und Robustheit, sind jedoch rechenintensiver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI2jJfOa-g8J"
      },
      "outputs": [],
      "source": [
        "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgxCZHE1-8Uc"
      },
      "outputs": [],
      "source": [
        "face_img = denis.copy()\n",
        "\n",
        "face_rects = face_cascade.detectMultiScale(face_img)\n",
        "\n",
        "for (x,y,w,h) in face_rects:\n",
        "  cv2.rectangle(face_img, (x,y), (x+w,y+h), 255, 10)\n",
        "\n",
        "plt.imshow(face_img,cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWn_-2yNACJY"
      },
      "outputs": [],
      "source": [
        "face_img = nadia.copy()\n",
        "\n",
        "face_rects = face_cascade.detectMultiScale(face_img)\n",
        "\n",
        "for (x,y,w,h) in face_rects:\n",
        "  cv2.rectangle(face_img, (x,y), (x+w,y+h), 255, 10)\n",
        "\n",
        "plt.imshow(face_img,cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6lD5ukc_Ron"
      },
      "outputs": [],
      "source": [
        "face_img = solvay.copy()\n",
        "\n",
        "face_rects = face_cascade.detectMultiScale(face_img)\n",
        "\n",
        "for (x,y,w,h) in face_rects:\n",
        "  cv2.rectangle(face_img, (x,y), (x+w,y+h), 255, 10)\n",
        "\n",
        "plt.imshow(face_img,cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV6ciW5d_x4Q"
      },
      "source": [
        "**Anpassung der Parameter bei Haar-Cascade-Erkennung**\n",
        "\n",
        "Die Gesichtserkennung mit Haar-Cascade kann durch das Anpassen bestimmter Parameter optimiert werden. Die wichtigsten Parameter sind:\n",
        "\n",
        "- scaleFactor: gibt an, wie stark das Bild bei jedem Schritt der Skalierung reduziert wird. Typischer Wert: 1.1 bis 1.5. Kleinere Werte erhöhen die Genauigkeit, aber auch die Berechnungszeit.\n",
        "- minNeighbors: bestimmt, wie viele benachbarte Regionen als positiv erkannt werden müssen, um als Gesicht zu gelten. Höhere Werte reduzieren Fehlalarme, können jedoch Gesichter übersehen. Typischer Wert: 3 bis 6.\n",
        "- minSize: legt die Mindestgröße eines zu erkennenden Objekts fest (Breite x Höhe). Verhindert die Erkennung kleiner, irrelevanter Objekte.\n",
        "- maxSize: optional, definiert die maximale Größe eines zu erkennenden Objekts. Nützlich, um große, irrelevante Bereiche auszuschließen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOxq45uc_mmw"
      },
      "outputs": [],
      "source": [
        "face_img = solvay.copy()\n",
        "\n",
        "face_rects = face_cascade.detectMultiScale(face_img,scaleFactor=1.2, minNeighbors=5)\n",
        "\n",
        "for (x,y,w,h) in face_rects:\n",
        "  cv2.rectangle(face_img, (x,y), (x+w,y+h), 255, 10)\n",
        "\n",
        "plt.imshow(face_img,cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNkCO4QAAnCf"
      },
      "source": [
        "**Smile Detektor**\n",
        "Der Smile-Detektor in OpenCV basiert auf der Haar-Cascade-Datei haarcascade_smile.xml. Er erkennt Lächeln, indem er charakteristische Muster im Bereich des Mundes analysiert.\n",
        "Der Smile-Detektor ist weniger robust als die Gesichtserkennung und anfällig für Variationen wie Licht, Winkel oder verdeckte Gesichter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJG4IFVNAlhQ"
      },
      "outputs": [],
      "source": [
        "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
        "eye_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_eye.xml')\n",
        "smile_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_smile.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DClFb6gtA4B_"
      },
      "outputs": [],
      "source": [
        "nadia = cv2.imread('Nadia_Murad.jpg')\n",
        "img = cv2.cvtColor(nadia, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gvirc1F9BQou"
      },
      "outputs": [],
      "source": [
        "img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "faces = face_cascade.detectMultiScale(img_gray)\n",
        "for (x, y, w, h) in faces:\n",
        "  # Ausschnitte kopieren\n",
        "  roi_gray = img_gray[y:y+h, x:x+w]\n",
        "  roi_color = img[y:y+h, x:x+h]\n",
        "  # Gesicht markieren\n",
        "  cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "  # Nach Augen suchen\n",
        "  eyes = eye_cascade.detectMultiScale(roi_gray, 1.1, 22)\n",
        "  for (ex, ey, ew, eh) in eyes:\n",
        "    cv2.rectangle(roi_color, (ex, ey),(ex+ew, ey+eh), (0, 255, 0), 2)\n",
        "\n",
        "  # Nach Smile suchen\n",
        "  smiles = smile_cascade.detectMultiScale(roi_gray, 1.7, 22)\n",
        "  for (sx, sy, sw, sh) in smiles:\n",
        "    cv2.rectangle(roi_color, (sx, sy),(sx+sw, sy+sh), (0, 0, 255), 2)\n",
        "\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2nmpPyYB7JW"
      },
      "outputs": [],
      "source": [
        "therock = cv2.imread('therock.jpg')\n",
        "img = cv2.cvtColor(therock, cv2.COLOR_BGR2RGB)\n",
        "img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "faces = face_cascade.detectMultiScale(img_gray)\n",
        "for (x, y, w, h) in faces:\n",
        "  # Ausschnitte kopieren\n",
        "  roi_gray = img_gray[y:y+h, x:x+w]\n",
        "  roi_color = img[y:y+h, x:x+h]\n",
        "  # Gesicht markieren\n",
        "  cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "  # Nach Augen suchen\n",
        "  eyes = eye_cascade.detectMultiScale(roi_gray, 1.1, 22)\n",
        "  for (ex, ey, ew, eh) in eyes:\n",
        "    cv2.rectangle(roi_color, (ex, ey),(ex+ew, ey+eh), (0, 255, 0), 2)\n",
        "\n",
        "  # Nach Smile suchen\n",
        "  smiles = smile_cascade.detectMultiScale(roi_gray, 1.7, 22)\n",
        "  for (sx, sy, sw, sh) in smiles:\n",
        "    cv2.rectangle(roi_color, (sx, sy),(sx+sw, sy+sh), (0, 0, 255), 2)\n",
        "\n",
        "plt.imshow(img)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrqYjZtjFyks"
      },
      "source": [
        "**Facial Landmarks mit Dlib**\n",
        "\n",
        "Dlib bietet eine robuste Methode zur Erkennung von Gesichtslandmarks, also charakteristischen Punkten im Gesicht (z. B. Augen, Nase, Mund, Kinn). Das Modell identifiziert standardmäßig 68 Landmark-Punkte, die präzise die Gesichtskonturen markieren.\n",
        "\n",
        "Funktionsweise:\n",
        "- Nach der Gesichtserkennung lokalisiert Dlib mithilfe eines vortrainierten Modells (z. B. shape_predictor_68_face_landmarks.dat) die Landmark-Punkte.\n",
        "- Diese Punkte werden genutzt, um Gesichtszüge für Anwendungen wie Gesichtsausrichtung, Mimik-Analyse oder Augmented Reality zu verfolgen.\n",
        "\n",
        "Vorteile:\n",
        "- Präzise und robust bei unterschiedlichen Gesichtswinkeln.\n",
        "- Funktioniert gut unter moderaten Lichtbedingungen.\n",
        "\n",
        "Nachteile:\n",
        "- Höherer Rechenaufwand im Vergleich zu Haar-Cascades.\n",
        "- Kann bei extremen Gesichtspositionen oder verdeckten Bereichen an Genauigkeit verlieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1xoJR7gF0yO"
      },
      "outputs": [],
      "source": [
        "import dlib\n",
        "import imutils\n",
        "from imutils import face_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N7kbDhwj__C"
      },
      "source": [
        "Zunächst müssen wir die Modelldaten herunterladen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLCHvWTvF6zP"
      },
      "outputs": [],
      "source": [
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbCLpkdsGGas"
      },
      "outputs": [],
      "source": [
        "PREDICTOR_PATH = \"shape_predictor_68_face_landmarks.dat\"\n",
        "predictor = dlib.shape_predictor(PREDICTOR_PATH)\n",
        "detector = dlib.get_frontal_face_detector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4rL9I3jGKwL"
      },
      "outputs": [],
      "source": [
        "img = dlib.load_rgb_image('Nadia_Murad.jpg')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNoaVIFNGXii"
      },
      "outputs": [],
      "source": [
        "rects = detector(img, 1)\n",
        "print(\"Number of faces detected: {}\".format(len(rects)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0wgCI9sGb3-"
      },
      "outputs": [],
      "source": [
        "# loop over the face detections\n",
        "for (i, rect) in enumerate(rects):\n",
        "\t# determine the facial landmarks for the face region, then\n",
        "\t# convert the facial landmark (x, y)-coordinates to a NumPy\n",
        "\t# array\n",
        "\tshape = predictor(img, rect)\n",
        "\tshape = face_utils.shape_to_np(shape)\n",
        "\t# convert dlib's rectangle to a OpenCV-style bounding box\n",
        "\t# [i.e., (x, y, w, h)], then draw the face bounding box\n",
        "\t(x, y, w, h) = face_utils.rect_to_bb(rect)\n",
        "\tcv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\t# show the face number\n",
        "\tcv2.putText(img, \"Face #{}\".format(i + 1), (x - 10, y - 10),\n",
        "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\t# loop over the (x, y)-coordinates for the facial landmarks\n",
        "\t# and draw them on the image\n",
        "\tfor (x, y) in shape:\n",
        "\t\tcv2.circle(img, (x, y), 1, (0, 0, 255), -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsK0bCH_GzRp"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67PbPk1eHdOO"
      },
      "source": [
        "\n",
        "**Mediapipe Hands**\n",
        "\n",
        "updated: 2024-01-22\n",
        "\n",
        "MediaPipe Hands ist ein Framework zur Handerkennung und -verfolgung in Echtzeit, das die Position von 21 Landmarks pro Hand präzise erkennt. Es kombiniert eine effiziente Handsegmentierung mit Landmark-Vorhersagen, die für Anwendungen wie Gestenerkennung oder Interaktionen in AR/VR nützlich sind.\n",
        "\n",
        "Usage example of MediaPipe Hands Solution API in Python (see also http://solutions.mediapipe.dev/hands).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRofPR6DHd9t"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jcl6MJvH29V"
      },
      "outputs": [],
      "source": [
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH  = 480\n",
        "def resize_and_show(image):\n",
        "  h, w = image.shape[:2]\n",
        "  if h < w:\n",
        "    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
        "  else:\n",
        "    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "  cv2_imshow(img)\n",
        "\n",
        "# Read images with OpenCV.\n",
        "file_names = ['hands1.jpg', 'hands2.jpg']\n",
        "\n",
        "# Read images with OpenCV.\n",
        "images = {name: cv2.imread(name) for name in file_names}\n",
        "\n",
        "# Preview the images.\n",
        "for name, image in images.items():\n",
        "  print(name)\n",
        "  resize_and_show(image)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivxInz1eKKp6"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "help(mp_hands.Hands)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ehx1VGseKWey"
      },
      "outputs": [],
      "source": [
        "# Run MediaPipe Hands.\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.7) as hands:\n",
        "\n",
        "  for name, image in images.items():\n",
        "    # Convert the BGR image to RGB, flip the image around y-axis for correct\n",
        "    # handedness output and process it with MediaPipe Hands.\n",
        "    results = hands.process(cv2.flip(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1))\n",
        "\n",
        "    # Print handedness (left v.s. right hand).\n",
        "    print(f'Handedness of {name}:')\n",
        "    print(results.multi_handedness)\n",
        "\n",
        "    if not results.multi_hand_landmarks:\n",
        "      continue\n",
        "    # Draw hand landmarks of each hand.\n",
        "    print(f'Hand landmarks of {name}:')\n",
        "    image_hight, image_width, _ = image.shape\n",
        "    annotated_image = cv2.flip(image.copy(), 1)\n",
        "    for hand_landmarks in results.multi_hand_landmarks:\n",
        "      # Print index finger tip coordinates.\n",
        "      print(\n",
        "          f'Index finger tip coordinate: (',\n",
        "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
        "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_hight})'\n",
        "      )\n",
        "      mp_drawing.draw_landmarks(\n",
        "          annotated_image,\n",
        "          hand_landmarks,\n",
        "          mp_hands.HAND_CONNECTIONS,\n",
        "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "          mp_drawing_styles.get_default_hand_connections_style())\n",
        "    resize_and_show(cv2.flip(annotated_image, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqL9xyiBLqsV"
      },
      "outputs": [],
      "source": [
        "# Run MediaPipe Hands and plot 3d hands world landmarks.\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.7) as hands:\n",
        "  for name, image in images.items():\n",
        "    # Convert the BGR image to RGB and process it with MediaPipe Hands.\n",
        "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    # Draw hand world landmarks.\n",
        "    print(f'Hand world landmarks of {name}:')\n",
        "    if not results.multi_hand_world_landmarks:\n",
        "      continue\n",
        "    for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
        "      mp_drawing.plot_landmarks(\n",
        "        hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLnJuFujNO-I"
      },
      "source": [
        "\n",
        "**MobileNet - SSD**\n",
        "\n",
        "updated: 2024-01-22\n",
        "\n",
        "MobileNet-SSD (Single Shot MultiBox Detector) ist ein leistungsstarkes und leichtgewichtiges Modell für die Objekterkennung in Echtzeit. Es kombiniert die Effizienz der MobileNet-Architektur mit dem SSD-Framework, das speziell für ressourcenschwache Geräte wie Smartphones oder eingebettete Systeme optimiert wurde.\n",
        "Eigenschaften\n",
        "- MobileNet: Eine effiziente Convolutional Neural Network (CNN)-Architektur, die Tiefe und Breite reduziert, ohne die Erkennungsgenauigkeit wesentlich zu beeinträchtigen.\n",
        "- SSD: Ermöglicht die direkte Erkennung von Objekten verschiedener Größen aus Feature-Maps und liefert sowohl Klassen- als auch Positionsvorhersagen in einem Schritt.\n",
        "\n",
        "Standardauflösung\n",
        "- 300x300 Pixel: die meisten vortrainierten MobileNet-SSD-Modelle, wie die, die mit TensorFlow oder OpenCV verwendet werden, sind auf Eingabebilder mit einer Größe von 300x300 Pixel optimiert.\n",
        "- optional: 512x512 Pixel: einige Modelle unterstützen höhere Auflösungen für verbesserte Genauigkeit, insbesondere bei kleinen oder entfernten Objekten. Dies geht jedoch zulasten der Verarbeitungsgeschwindigkeit.\n",
        "- die Eingabebilder werden automatisch auf die Modellauflösung skaliert, z. B. auf 300x300 oder 512x512 Pixel, unabhängig von der ursprünglichen Bildgröße.\n",
        "\n",
        "Vorteile\n",
        "- Echtzeitfähigkeit: Ideal für Anwendungen mit niedriger Latenz.\n",
        "- Ressourcenschonend: Funktioniert auf mobilen Geräten mit begrenzter Rechenleistung.\n",
        "- Multi-Objekt-Erkennung: Erkennt mehrere Objekte gleichzeitig in einem Bild.\n",
        "\n",
        "Anwendungen\n",
        "- MobileNet-SSD wird häufig für Echtzeit-Detektion in Bereichen wie Überwachungskameras, Augmented Reality und Robotik eingesetzt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNgto323NQNR"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/chuanqi305/MobileNet-SSD/  # clone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TphkRBVBNa9f"
      },
      "outputs": [],
      "source": [
        "proto_file = \"MobileNet-SSD/deploy.prototxt\"\n",
        "model_file = \"MobileNet-SSD/mobilenet_iter_73000.caffemodel\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZEaWuF5Nf6a"
      },
      "outputs": [],
      "source": [
        "net = cv2.dnn.readNetFromCaffe(proto_file,model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H79oq_gVNjnc"
      },
      "outputs": [],
      "source": [
        "#——Class Labels of the model——–#\n",
        "\n",
        "classNames = { 0: 'background',\n",
        "\n",
        "    1: 'aeroplane', 2: 'bicycle', 3: 'bird', 4: 'boat',\n",
        "\n",
        "    5: 'bottle', 6: 'bus', 7: 'car', 8: 'cat', 9: 'chair',\n",
        "\n",
        "    10: 'cow', 11: 'diningtable', 12: 'dog', 13: 'horse',\n",
        "\n",
        "    14: 'motorbike', 15: 'person', 16: 'pottedplant',\n",
        "\n",
        "    17: 'sheep', 18: 'sofa', 19: 'train', 20: 'tvmonitor' }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnIl_jigNpyo"
      },
      "outputs": [],
      "source": [
        "input_shape=(300,300) #the required shape for the input image to pass to our model\n",
        "\n",
        "mean = (127.5,127.5,127.5) #we’ll have to normalize the image pixels, and we’ll use this mean value to do that\n",
        "\n",
        "scale = 0.007843 # then finally we’ll scale the image to meet the input criteria of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkqfU9LANtCL"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"Dog_Cat.jpg\")\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwPLIzGTNwIQ"
      },
      "outputs": [],
      "source": [
        "blob = cv2.dnn.blobFromImage(img, scalefactor=scale, size=input_shape, mean=mean,\n",
        "        swapRB=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPzaK8muOBRH"
      },
      "outputs": [],
      "source": [
        "net.setInput(blob)\n",
        "\n",
        "#—–using the model to make predictions\n",
        "\n",
        "results = net.forward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI1tPFrzOGiC"
      },
      "outputs": [],
      "source": [
        "for i in range(results.shape[2]):\n",
        "\n",
        "      # confidence\n",
        "    confidence = round(results[0, 0, i, 2],2)\n",
        "    if confidence > 0.7:\n",
        "\n",
        "          # class id\n",
        "        id = int(results[0, 0, i, 1])\n",
        "\n",
        "        # 3-6 contains the coordinate\n",
        "        x1, y1, x2, y2 = results[0, 0, i, 3:7]\n",
        "\n",
        "        # print(x1,y1,x2,y2)\n",
        "        # scale these coordinates to out image pixel\n",
        "        ih, iw, ic = img.shape\n",
        "        x1, x2 = int(x1*iw), int(x2*iw)\n",
        "        y1, y2 = int(y1 * ih), int(y2 * ih)\n",
        "        cv2.rectangle(img,\n",
        "                      (x1, y1),\n",
        "                      (x2, y2),\n",
        "                      (0, 255, 0), 2)\n",
        "        cv2.putText(img, f'{classNames[id]}: {confidence:.2f}',\n",
        "                    (x1 + 30, y1 + 30),\n",
        "                    cv2.FONT_HERSHEY_DUPLEX,\n",
        "                    1, (255, 0, 0), 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x39fBCZpOKU9"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4DUwy7dPpLj"
      },
      "source": [
        "**Deep Learning with YOLOv5**\n",
        "\n",
        "updated 2024-01-22\n",
        "\n",
        "YOLOv5 (You Only Look Once, Version 5) ist ein modernes, schnelles und genaues Framework für die Echtzeit-Objekterkennung. Es wurde von Ultralytics entwickelt und zeichnet sich durch seine leichte Bedienbarkeit, hohe Geschwindigkeit und Flexibilität aus. YOLOv5 ist eine Weiterentwicklung der vorherigen YOLO-Versionen und bietet signifikante Verbesserungen bei der Effizienz und Genauigkeit. Es wird häufig in Anwendungen eingesetzt, die schnelles und präzises Erkennen von Objekten in Bildern oder Videos erfordern, z. B. Überwachungssysteme, autonomes Fahren, Industrieautomatisierung und Augmented Reality.\n",
        "\n",
        "Hauptmerkmale von YOLOv5\n",
        "-Echtzeitfähigkeit: Modelle liefern eine Verarbeitungsgeschwindigkeit von über 30 FPS (Frames per Second).\n",
        "- Plattformübergreifend: kann sowohl auf CPU als auch GPU ausgeführt werden, wobei CUDA für GPU-Beschleunigung unterstützt wird.\n",
        "- Erweiterbarkeit: ermöglicht Transfer Learning, um vortrainierte Modelle an spezifische Anwendungen anzupassen.\n",
        "- Onnx- und TensorRT-Export: Unterstützt die Umwandlung der Modelle für Plattformen wie Edge-Devices.\n",
        "\n",
        "Schritte der YOLO-Architektur:\n",
        "- Bildteilung in Raster: das Bild wird in ein Raster (z. B. 13x13) aufgeteilt. Jede Zelle ist dafür verantwortlich, Objekte zu erkennen, deren Mittelpunkt in der Zelle liegt.\n",
        "- Vorhersage pro Rasterzelle: jede Rasterzelle gibt für eine festgelegte Anzahl von Bounding-Boxen Vorhersagen aus:\n",
        "  - Objektklasse(n): Gibt an, welches Objekt (z. B. Auto, Hund) sich in der Box befindet.\n",
        "  - Positionswerte: Die Koordinaten (x, y, Breite, Höhe) der Box im Bild.\n",
        "  - Konfidenzwert: Gibt an, wie sicher das Modell ist, dass sich ein Objekt in der Box befindet.\n",
        "- Bounding Box Regression: YOLO optimiert die Größe und Position der Bounding-Boxen basierend auf den Trainingsdaten.\n",
        "\n",
        "- Non-Maximum Suppression (NMS): überlappende Boxen, die dasselbe Objekt beschreiben, werden anhand des Konfidenzwerts zusammengeführt, um Mehrfacherkennungen zu vermeiden.\n",
        "\n",
        "Neben YOLOv5 gibt es die neueren Modelle YOLOv6, YOLOv7, YOLOv8 ..., die noch höhere Leistung und zusätzliche Funktionen bieten, wie eine verbesserte Architektur und integrierte Tools für Aufgaben wie Instanzsegmentierung oder Pose-Erkennung.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDAGeFQnl1Ry"
      },
      "source": [
        "Clone GitHub repository, install dependencies and check PyTorch and GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jusoC-AyPp6P"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt comet_ml  # install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ySp3O8gP5GL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxa2K2KWQAk5"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images\n",
        "display.Image(filename='runs/detect/exp/bus.jpg', width=600)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}